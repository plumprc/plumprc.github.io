---
title: 舰船检测项目指导书（持续更新中）
date: 2019-08-30 20:10:54
categories:
- 机器学习
tags:
- 机器学习
- kaggle
---

![process.png](https://i.loli.net/2020/01/28/w7FgiTANelGsDjd.png)


# 前言
&emsp;&emsp;本指导书的目的是帮助读者快速了解并熟悉机器学习/深度学习相关项目的流程，建议读者在阅读前至少掌握一门计算机编程语言（如python），且已经了解机器学习的部分概念。本文的写作源于笔者在该项目中的实际经验，如有不足之处还请斧正。<br>
* [机器学习入门](http://www.ai-start.com/)
* [舰船检测项目地址](https://github.com/cxdzb/Airbus-Ship-Detection)

# 项目分析
![ships_xs.jpg](https://i.loli.net/2020/01/29/UmxKLvPsE23NdbA.jpg)
## 需求
&emsp;&emsp;我们需要在给定的卫星遥感图像中精确定位船只（找到船只并给出它的像素位置），用mask掩码来表示检测到的目标，如下图所示
![RLE.png](https://i.loli.net/2020/01/29/QWFLbvU4YK2ZwxJ.png)
## 问题类型
&emsp;&emsp;这是一个典型的图像分割问题，这说明我们的预选模型为图像分割领域的SOFA（state-of-the-art）模型，例如mask-RCNN模型，U-net模型等
> * 注意！千万不要简单地通过问题类型来直接选择模型，在实践中模型的确立往往是由问题类型和数据集类型两方面因素决定的

# 数据探索和可视化（EDA）
> Q : 为什么我们要进行EDA?<br>
> A : 一般来说我们获得的数据集都比较庞大且杂乱，我们不可能人工去仔细查看每张图片的类型和特征呈现效果如何，我们必须借助抽样的方式让程序以可视化的形式告诉我们数据集的特征，以此为依据决定后面的工作（数据清洗、数据预处理等），因此EDA是进行数据处理前必不可少的一环


## 相关包的引入


```python
import numpy as np # linear computation
import pandas as pd # data processing, CSV file I/O
from skimage.data import imread # read image
import matplotlib.pyplot as plt # plot
import os # path
from pathlib import Path
PATH = 'F:/[Base] Code/DataSets/airbus-ship-detection'
```

## 查看数据集格式
&emsp;&emsp;大多数带标签数据集都以csv（表格）格式呈现，如下图所示，利用pandas包读取csv数据集得到数据表，格式为tuple（obj, features），这里即图片（ImageId）和图片对应的船只位置信息（EncodeedPixels）


```python
train = pd.read_csv(PATH + '/train_ship_segmentations_v2.csv')
train.sample(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ImageId</th>
      <th>EncodedPixels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>168038</th>
      <td>b95d7e978.jpg</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>136569</th>
      <td>96bbbb8b8.jpg</td>
      <td>118372 1 119138 4 119905 5 120672 7 121438 10 ...</td>
    </tr>
    <tr>
      <th>107214</th>
      <td>75fe4b3c8.jpg</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



&emsp;&emsp;因为图片的位置信息编码格式为run-length编码（目的是压缩以减小传输尺寸，一般较大的数据集的标签都不会完整呈现给我们而是以一种压缩编码的方式呈现，我们需要将其解码才能得到完整信息，以下为rle解码）


```python
# reference: https://www.kaggle.com/paulorzp/run-length-encode-and-decode
def rle_decode(mask_rle, shape=(768, 768)):
    '''
    mask_rle: run-length as string formated (start length)
    shape: (height,width) of array to return 
    Returns numpy array, 1 - mask, 0 - background

    '''
    s = mask_rle.split()
    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]
    starts -= 1
    ends = starts + lengths
    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)
    for lo, hi in zip(starts, ends):
        img[lo:hi] = 1
    return img.reshape(shape).T  # Needed to align to RLE direction
```

## 图片展示（有船）


```python
sample = train[~train.EncodedPixels.isna()].sample(15)

fig, ax = plt.subplots(3, 5, sharex='col', sharey='row')
fig.set_size_inches(20, 12)

for i, imgid in enumerate(sample.ImageId):
    col = i % 5
    row = i // 5
    
    path = Path(PATH + '/train_v2') / '{}'.format(imgid)
    img = imread(path)
    
    ax[row, col].imshow(img)
```


![EDA_7_0.png](https://i.loli.net/2020/01/29/5bzYXoH37sSg8TM.png)


## RLE解码信息


```python
masks = pd.read_csv(PATH + '/train_ship_segmentations_v2.csv')
fig, ax = plt.subplots(3, 5, sharex='col', sharey='row')
fig.set_size_inches(20, 12)
all_masks = np.zeros((768, 768)) # purple
for i, imgid in enumerate(sample.ImageId):
    col = i % 5
    row = i // 5
    img = imread(PATH + '/train_v2/' + imgid)
    img_masks = masks.loc[masks['ImageId'] == imgid, 'EncodedPixels'].tolist()
    for mask in img_masks:
        all_masks += rle_decode(mask)
    ax[row, col].imshow(all_masks)
    all_masks = np.zeros((768, 768))
```


![EDA_9_0.png](https://i.loli.net/2020/01/29/wQZmjkUWztpv2Iy.png)


&emsp;&emsp;我们随机抽选了15张有船的图片（可重复随机抽选），解码后图片为根据数据集标签得出的船只像素位置。我们发现部分船只的位置信息是很清晰的，解码后的黄色区域也是独立且清晰，但部分图片存在船只过小或不清楚的情况，对应至解码后图片的表现为过小的船只以点状分布，有的稀疏有的密集，特征混乱且不明显；此外大小不一的船只解码区域的交界处也不明显，存在重叠现象

![normal-vs-mask.gif](https://i.loli.net/2020/01/29/6O4RBWiArDpNfEz.gif)

## 图片展示（无船）


```python
sample = train[train.EncodedPixels.isna()].sample(15)

fig, ax = plt.subplots(3, 5, sharex='col', sharey='row')
fig.set_size_inches(20, 12)

for i, imgid in enumerate(sample.ImageId):
    col = i % 5
    row = i // 5
    
    path = Path(PATH + '/train_v2') / '{}'.format(imgid)
    img = imread(path)
    
    ax[row, col].imshow(img)
```


![EDA_12_0.png](https://i.loli.net/2020/01/29/FVG1uR5zSm9C7BZ.png)


&emsp;&emsp;我们再随机抽选15张无船的图片，可以看到有没有干扰的图片（纯色海水，大片白云），但也存在干扰较为明显的图片（云层，港口，縠纹），这对我们检测船只的特征增加了难度

## 数据集类型比较


```python
ships = train[~train.EncodedPixels.isna()].ImageId.unique()
noships = train[train.EncodedPixels.isna()].ImageId.unique()

plt.bar(['Ships', 'No Ships'], [len(ships), len(noships)]);
plt.ylabel('Number of Images');
```


![EDA_15_0.png](https://i.loli.net/2020/01/29/WH6ChO8mZM4JUNr.png)


&emsp;&emsp;可以看出无船的负样本数目远大于有船的数目，因此有必要在训练前对样本分布进行处理

## 单图片船只数目统计


```python
unique_img_ids = masks.groupby('ImageId').size().reset_index(name='counts')
train_df = pd.merge(masks, unique_img_ids)
print(train_df.shape[0], 'training masks')
train_df['counts'] = train_df.apply(lambda c_row: c_row['counts'] if isinstance(
    c_row['EncodedPixels'], str
) else 0, 1)
train_df.head()
```

    231723 training masks
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ImageId</th>
      <th>EncodedPixels</th>
      <th>counts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>00003e153.jpg</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0001124c7.jpg</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000155de5.jpg</td>
      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000194a2d.jpg</td>
      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000194a2d.jpg</td>
      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_df['counts'].hist(bins = 30)
```

![EDA_19_1.png](https://i.loli.net/2020/01/29/JQRp6sc8LWxubEO.png)


&emsp;&emsp;可以看出，约有十五万张没有船只的负样本，这占据了绝大部分样本空间；剩下的有船图片里，单单船和双船模型占据了绝大部分的空间，而多船模型在样本空间内仅占一小部分。我们的特征学习主要来源于有船样本标签提供的特征，因此在排除无船样本的前提下，我们的重心可以放在单船和双船模型的检测上，所以样本均衡是之后必不可少的一个环节

## 图片颜色特征分布


```python
def get_img(imgid):
    '''Return image array, given ID'''
    path = Path(PATH + '/train_v2') / '{}'.format(imgid)
    return imread(path)
```


```python
fig, ax = plt.subplots(1, 2, sharex='col', sharey='row')
fig.set_size_inches(20, 6)

mask = train.EncodedPixels.isna()
for i, (msk, label) in enumerate(zip([mask, ~mask], ['No Ships', 'Ships'])):
    _ids = train[msk].ImageId.sample(250)
    imgs = np.array([get_img(_id) for _id in _ids])
    
    red = imgs[:, :, :, 0]
    green = imgs[:, :, :, 1]
    blue = imgs[:, :, :, 2]
    
    ax[i].plot(np.bincount(red.ravel()), color='orangered', label='red', lw=2)
    ax[i].plot(np.bincount(green.ravel()), color='yellowgreen', label='green', lw=2)
    ax[i].plot(np.bincount(blue.ravel()), color='skyblue', label='blue', lw=2)
    ax[i].legend()
    ax[i].title.set_text(label)
```


![EDA_23_0.png](https://i.loli.net/2020/01/29/JAxS4i92fHhudVP.png)


&emsp;&emsp;除了标签提供的位置特征外，对于图像型数据来说，颜色特征是一个值得考虑的隐藏特征。经分析我们可以看出，有船和无船样本图片的颜色特征基本没有差别，因此我们排除了添加特征颜色作为识别的依据这一可能性

## 标签区域颜色特征分布


```python
def apply_masks_to_img(img, _id, df):
    '''Apply masks to image given img, its id and the dataframe.'''
    masks = df[df.ImageId == _id].EncodedPixels.apply(lambda x: rle_decode(x)).tolist()
    masks = sum(masks)
    return img * masks.reshape(img.shape[0], img.shape[1], 1)


fig, ax = plt.subplots(1, 2, sharex='col')#, sharey='row')
fig.set_size_inches(20, 6)

mask = train.EncodedPixels.isna()
for i, (msk, label) in enumerate(zip([mask, ~mask], ['No Ships', 'Ships'])):
    _ids = train[msk].ImageId.sample(250)
    imgs = [get_img(_id) for _id in _ids]
    
    # if we have an encoding to decode
    if i == 1:
        imgs = [apply_masks_to_img(i, _id, train) for (i, _id) in zip(imgs, _ids)]

    imgs = np.array(imgs)
    red = imgs[:, :, :, 0]
    green = imgs[:, :, :, 1]
    blue = imgs[:, :, :, 2]
    
    # skip bincount index 0 to avoid the masked pixels to overpower the others.
    ax[i].plot(np.bincount(red.ravel())[1:], color='orangered', label='red', lw=2)
    ax[i].plot(np.bincount(green.ravel())[1:], color='yellowgreen', label='green', lw=2)
    ax[i].plot(np.bincount(blue.ravel())[1:], color='skyblue', label='blue', lw=2)
    ax[i].legend()
    ax[i].title.set_text(label)
```


![EDA_26_0.png](https://i.loli.net/2020/01/29/7vapKFzOgeR9kf1.png)


&emsp;&emsp;很明显，标签区域附近有船样本和无船样本的颜色特征差异较为明显，但是这里的颜色差异是由标签引起的，即有船图片标注的的船只部分颜色一定与无船图片有较大差异，这一部分的特征学习属于标签学习的范畴。至此，我们基本排除了以特征颜色为参考的可能性

> &emsp;&emsp;以上就是本项目中的EDA工作。无论你在做什么项目，EDA都是处理数据前必不可少的一环，它能帮助你全局浏览数据集的状况，对后续处理有着很重要的参考作用（例如需不需要进行数据清洗，需不需要进行数据增强等操作）。熟练掌握pandas、matplotlib等读写数据集和绘图的python包对你的工作将起到很大的帮助
* [python-pandas](https://www.jianshu.com/p/be029b1edc38)
* [python-matplotlib](https://www.matplotlib.org.cn/)

## 补充：没有数据的情况
&emsp;&emsp;一般来说没有数据集的话需要自行整理（拍照、统计）数据集，人工标注的工作量往往比较大，这时候你可以选择使用公开的数据集，一般来说很多计算、竞赛类平台会为大众提供自带标签的公开数据集，以下为几个来源参考
* [遥感图像](https://sandbox.intelligence-airbusds.com/web/)
* [交通标志牌](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)
* [人脸识别](https://github.com/polarisZhao/awesome-face)

# 模型选择
&emsp;&emsp;一般来说我们会根据问题类型和数据情况决定使用哪种SOFA模型。很明显这是一个图像分割问题，且我们的数据集分布比较有特点，分割的主体——船只在数据集中大多以单船和双船（分散居多）出现，多船的情况占少数，识别目标较为单一，因此我们可以考虑把这个问题看做为语义分割，即对输入图像中的每个像素分配一个语义类别，以得到像素化的密集分类。而在语义分割这个方向的SOFA模型当属以ResNet为基础的U-Net模型，因此最终我们敲定了选择U-Net模型作为我们的backbone
* [语义分割](https://blog.csdn.net/yunqiinsight/article/details/82968440)
* [U-net]()

# 数据处理
## 数据均衡
&emsp;&emsp;EDA后我们很明显观察到，我们的训练样本里存在着大量的负样本（无船样本），这意味着我们在训练模型时必须丢弃一部分负样本以确保神经网络可以提取足够多的特征

```python
print(masks.shape[0])
masks = masks.drop(masks[masks.EncodedPixels.isnull()].sample(150000,random_state=42).index)
print(masks.shape[0])
masks.sample(3)
```

    231723
    81723

&emsp;&emsp;如上，我们丢弃了十五万个负样本

## 交叉验证集
&emsp;&emsp;为了给模型评估建立一个独立的评估样本集，我们需要把原始的有标签数据集按照一定比例（一般为7 : 3）分为训练集（trainign set）和验证集（validation set），训练集用于模型的训练而验证集用于验证模型效果。在本数据集上考虑到数据集比较大，为了加快验证时的反馈调整机制我们减小了验证集的分配比率，按照19 : 1的比率划分整个数据集
```python
from sklearn.model_selection import train_test_split
unique_img_ids = masks.groupby('ImageId').size().reset_index(name='counts')
train_ids, valid_ids = train_test_split(unique_img_ids, 
                 test_size = 0.05, 
                 stratify = unique_img_ids['counts'],
                 random_state=42
                )
train_df = pd.merge(masks, train_ids)
valid_df = pd.merge(masks, valid_ids)
print(train_df.shape[0], 'training masks')
print(valid_df.shape[0], 'validation masks')
train_df.head(3)
```

    77636 training masks
    4087 validation masks

## 数据增强(Data Augmentation)

```python
from keras.preprocessing.image import ImageDataGenerator
dg_args = dict(featurewise_center = False, 
                  samplewise_center = False,
                  rotation_range = 15, 
                  width_shift_range = 0.1, 
                  height_shift_range = 0.1, 
                  shear_range = 0.01,
                  zoom_range = [0.9, 1.25],  
                  horizontal_flip = True, 
                  vertical_flip = True,
                  fill_mode = 'reflect',
                   data_format = 'channels_last')
# brightness can be problematic since it seems to change the labels differently from the images 
if AUGMENT_BRIGHTNESS:
    dg_args[' brightness_range'] = [0.5, 1.5]
image_gen = ImageDataGenerator(**dg_args)

if AUGMENT_BRIGHTNESS:
    dg_args.pop('brightness_range')
label_gen = ImageDataGenerator(**dg_args)

def create_aug_gen(in_gen, seed = None):
    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))
    for in_x, in_y in in_gen:
        seed = np.random.choice(range(9999))
        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks
        g_x = image_gen.flow(255*in_x, 
                             batch_size = in_x.shape[0], 
                             seed = seed, 
                             shuffle=True)
        g_y = label_gen.flow(in_y, 
                             batch_size = in_x.shape[0], 
                             seed = seed, 
                             shuffle=True)

        yield next(g_x)/255.0, next(g_y)
```

    Using TensorFlow backend.

## 效果展示
```python
cur_gen = create_aug_gen(train_gen)
t_x, t_y = next(cur_gen)
print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())
print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())
# only keep first 9 samples to examine in detail
t_x = t_x[:9]
t_y = t_y[:9]
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))
ax1.imshow(montage_rgb(t_x), cmap='gray')
ax1.set_title('images')
ax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')
ax2.set_title('ships')
```

    x (4, 768, 768, 3) float32 0.0 1.0
    y (4, 768, 768, 1) float32 0.0 1.0
    Text(0.5,1,'ships')

![Augmentation](应有图)


  




# 模型搭建
## U-net Model


```python
class UNet_down_block(torch.nn.Module):
    def __init__(self, input_channel, output_channel, down_size):
        super(UNet_down_block, self).__init__()
        self.conv1 = torch.nn.Conv2d(input_channel, output_channel, 3, padding=1)
        self.bn1 = torch.nn.BatchNorm2d(output_channel)
        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)
        self.bn2 = torch.nn.BatchNorm2d(output_channel)
        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)
        self.bn3 = torch.nn.BatchNorm2d(output_channel)
        self.max_pool = torch.nn.MaxPool2d(2, 2)
        self.relu = torch.nn.ReLU()
        self.down_size = down_size

    def forward(self, x):
        if self.down_size:
            x = self.max_pool(x)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.relu(self.bn3(self.conv3(x)))
        return x

class UNet_up_block(torch.nn.Module):
    def __init__(self, prev_channel, input_channel, output_channel):
        super(UNet_up_block, self).__init__()
        self.up_sampling = torch.nn.Upsample(scale_factor=2, mode='bilinear')
        self.conv1 = torch.nn.Conv2d(prev_channel + input_channel, output_channel, 3, padding=1)
        self.bn1 = torch.nn.BatchNorm2d(output_channel)
        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)
        self.bn2 = torch.nn.BatchNorm2d(output_channel)
        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)
        self.bn3 = torch.nn.BatchNorm2d(output_channel)
        self.relu = torch.nn.ReLU()

    def forward(self, prev_feature_map, x):
        x = self.up_sampling(x)
        x = torch.cat((x, prev_feature_map), dim=1)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.relu(self.bn3(self.conv3(x)))
        return x


class UNet(torch.nn.Module):
    def __init__(self):
        super(UNet, self).__init__()

        self.down_block1 = UNet_down_block(3, 16, False)
        self.down_block2 = UNet_down_block(16, 32, True)
        self.down_block3 = UNet_down_block(32, 64, True)
        self.down_block4 = UNet_down_block(64, 128, True)
        self.down_block5 = UNet_down_block(128, 256, True)
        self.down_block6 = UNet_down_block(256, 512, True)
        self.down_block7 = UNet_down_block(512, 1024, True)

        self.mid_conv1 = torch.nn.Conv2d(1024, 1024, 3, padding=1)
        self.bn1 = torch.nn.BatchNorm2d(1024)
        self.mid_conv2 = torch.nn.Conv2d(1024, 1024, 3, padding=1)
        self.bn2 = torch.nn.BatchNorm2d(1024)
        self.mid_conv3 = torch.nn.Conv2d(1024, 1024, 3, padding=1)
        self.bn3 = torch.nn.BatchNorm2d(1024)

        self.up_block1 = UNet_up_block(512, 1024, 512)
        self.up_block2 = UNet_up_block(256, 512, 256)
        self.up_block3 = UNet_up_block(128, 256, 128)
        self.up_block4 = UNet_up_block(64, 128, 64)
        self.up_block5 = UNet_up_block(32, 64, 32)
        self.up_block6 = UNet_up_block(16, 32, 16)

        self.last_conv1 = torch.nn.Conv2d(16, 16, 3, padding=1)
        self.last_bn = torch.nn.BatchNorm2d(16)
        self.last_conv2 = torch.nn.Conv2d(16, 1, 1, padding=0)
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        self.x1 = self.down_block1(x)
        self.x2 = self.down_block2(self.x1)
        self.x3 = self.down_block3(self.x2)
        self.x4 = self.down_block4(self.x3)
        self.x5 = self.down_block5(self.x4)
        self.x6 = self.down_block6(self.x5)
        self.x7 = self.down_block7(self.x6)
        self.x7 = self.relu(self.bn1(self.mid_conv1(self.x7)))
        self.x7 = self.relu(self.bn2(self.mid_conv2(self.x7)))
        self.x7 = self.relu(self.bn3(self.mid_conv3(self.x7)))
        x = self.up_block1(self.x6, self.x7)
        x = self.up_block2(self.x5, x)
        x = self.up_block3(self.x4, x)
        x = self.up_block4(self.x3, x)
        x = self.up_block5(self.x2, x)
        x = self.up_block6(self.x1, x)
        x = self.relu(self.last_bn(self.last_conv1(x)))
        x = self.last_conv2(x)
        return x
```




