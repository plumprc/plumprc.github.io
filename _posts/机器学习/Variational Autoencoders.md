<center>PAPER: <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></center>

## Motivations
&emsp;&emsp;In generative modeling, we often need to get the distribution $P(x)$ of the data $x$. That is to say, if the $x$ looks like the real data, it should get high probability. If the $x$ looks like the random noise, it should get low probability. Formally speaking, real data $x$ is sampled from an unknown distribution $P_{gt}(x)$ and our goal is to learn a model $P$ which we can sample from, such that $P$ is as similar as possible to $P_{gt}$.

## Latent Variable Models
&emsp;&emsp;Considering the high dimension of the raw data $x$, we should embed $x$ into latent space $z$ which can easily sampled from $P(z)$. Assume we have a family of functions $f(z;\theta)$, where $\theta$ is learnable parameters. Optimize $\theta$ such that $f(z;\theta)$ can produce samples like $x$ with high probability.

$$\arg\max_\theta p(x)=\int p(x\vert z;\theta)\cdot p(z)\text{d}z$$

&emsp;&emsp;Here, $f(z;\theta)$ is replaced by $P(x\vert z;\theta)$ due to maximum likelihood. In VAEs, the choice of this output distribution is often Gaussian. You can also use other distribution but you should guarantee that $\theta$ is continuous.

## Variational Autoencoders
&emsp;&emsp;To optimize $P(x)$, there are two problems we should deal with:
* How to define the latent variables $z$?
* How to deal with the integral over $z$?

&emsp;&emsp;VAEs assume that there is no simple interpretation of the dimensions of $z$ and instead assert that samples of $z$ can be drawn from a simple distribution. In fact, any distribution in $d$ dimensions can be generated by taking $d$ variables which are normally distributed. Thus, we can choose $P(z)=N(0,1)$. Then what we should do is to sample a large number of $z$ values and compute $P(x)=\displaystyle\frac{1}{n}\sum_ip(x\vert z_i)$. The problem is that in high dimensional space, this way needs a large amount of samples. Another problem is that if $P(x\vert z)$ is an isotropic Gaussian, optimizing $P(x)$ is equal to minimize the squared Euclidean distance between $f(z)$ and $x$, which is not good in practice.

TODO: it's hard to find a better similarity metric

&emsp;&emsp;In practice, for most $z$, $P(x\vert z)$ will be nearly zero. The key idea behind VAEs is to attempt to sample values of $z$ that are likely to have produced $x$. This means we need a new function $Q(z\vert x)$ where we can get a distribution over $z$ values which are likely to produce $x$. The space of $z$ under $Q$ is much smaller than $z$ under $P(z)$. Then we should make $Q(z\vert x)$ and the true posterior distribution $P(z\vert x)$ more similar so that we can use $P(x)\approx\mathbb{E}_{z\sim Q}p(x\vert z)$ to estimate $P(x)$. Consider the KL divergence (denoted as $D$) between them:

$$D(Q(z\vert x),P(z\vert x))=\mathbb{E}_{z\sim Q}[\log Q(z\vert x)-\log P(z\vert x)]=\mathbb{E}_{z\sim Q}[\log Q(z\vert x)-\log P(x\vert z)-\log P(z)]+\log P(x)$$

$$\log P(x)-D(Q(z\vert x),P(z\vert x))=\mathbb{E}_{z\sim Q}\log P(x\vert z)-D(Q(z\vert x),P(z))$$

&emsp;&emsp;Our optimizing goal is to maximize $P(x)$ and minimize $D(Q(z\vert x),P(z\vert x))$. The right hand side looks like an autoencoder where $Q$ encodes $x$ to $z$ and $P$ decodes $z$ to $x$.

&emsp;&emsp;To apply SGD on the right hand side of above equation, we should specify all the terms. We know $P(z)=N(0,1)$ and $Q$ is often initialized as Gaussian with learnable mean and variance. The expectation $\mathbb{E}_{z\sim Q}\log P(x\vert z)$ can be estimated by [reparameterization trick]().

![VAE.png](https://s2.loli.net/2021/12/16/IrnsQz2dLAb47w8.png)

$$\nabla_\theta\mathbb{E}_{Q(z;\theta)}\log P(x\vert z)=\mathbb{E}_{\varepsilon\sim N(0,1)}\nabla_\theta\log P(x\vert f(\varepsilon;\theta))$$

