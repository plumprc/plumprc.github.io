<center>论文地址：<a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></center>

## Motivations
&emsp;&emsp;In generative modeling, we often need to get the distribution $p(x)$ of the data $x$. That is to say, if the $x$ looks like the real data, it should get high probability. If the $x$ looks like the random noise, it should get low probability. Formally speaking, real data $x$ is sampled from an unknown distribution $p_{gt}(x)$ and our goal is to learn a model $p$ which we can sample from, such that $p$ is as similar as possible to $p_{gt}$.

## Latent Variable Models
&emsp;&emsp;Considering the high dimension of the raw data $x$, we should embed $x$ into latent space $z$ which can easily sampled from $p(z)$. Assume we have a family of functions $f(z;\theta)$, where $\theta$ is learnable parameters. Optimize $\theta$ such that $f(z;\theta)$ can produce samples like $x$ with high probability.

$$\arg\max_\theta p(x)=\int p(x\vert z;\theta)\cdot p(z)\text{d}z$$

&emsp;&emsp;Here, $f(z;\theta)$ is replaced by $p(x\vert z;\theta)$ due to maximum likelihood. In VAEs, the choice of this output distribution is often Gaussian. You can also use other distribution but you should guarantee that $\theta$ is continuous.

## Variational Autoencoders
&emsp;&emsp;To optimize $p(x)$, there are two problems we should deal with:
* How to define the latent variables $z$?
* How to deal with the integral over $z$?

&emsp;&emsp;VAEs assume that there is no simple interpretation of the dimensions of $z$ and instead assert that samples of $z$ can be drawn from a simple distribution. In fact, any distribution in $d$ dimensions can be generated by taking $d$ variables which are normally distributed. Thus, we can choose $p(z)=N(0,1)$. Then what we should do is to sample a large number of $z$ values and compute $p(x)=\displaystyle\frac{1}{n}\sum_ip(x\vert z_i)$. The problem is that in high dimensional space, this way needs a large amount of samples. Another problem is that if $p(x\vert z)$ is an isotropic Gaussian, optimizing $p(x)$ is equal to minimize the squared Euclidean distance between $f(z)$ and $x$, which is not good in practice.

TODO: g(z) -> z / 10 + z / norm(z)

&emsp;&emsp;In practice, for most $z$, $p(x\vert z)$ will be nearly zero. The key idea behind VAEs is to attempt to sample values of $z$ that are likely to have produced $x$. Assume that $z$ is sampled from $Q(z)$ which is like posterior distribution. Then we can use $p(x)\approx\mathbb{E}_{z\sim Q}p(x\vert z)$ to estimate $p(x)$. 


https://blog.csdn.net/qq_27465499/article/details/86775426