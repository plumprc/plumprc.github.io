---
title: 《Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss》阅读笔记
date: 2021-08-13 21:31:27
categories:
- 机器学习
tags:
- 机器学习
- 表示学习
- 自监督学习
- 时间序列
---

<center>论文地址：<a href="https://arxiv.org/abs/2106.10466">Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss</a></center>

## Background and Motivation

* Defects in traditional time-series forecasting or classification approach：
  * Instance-level(describe the whole segment of the input time series) representations may not be suitable for tasks that require fine-grained representations
  * Few of the existing methods distinguish the multi-scale contextual information with different granularities. Neither of them featurizes time series at different scales to capture scale-invariant information
* Intuition: multi-scale features may provide different levels of semantics and improve the generalization capability of learned representations.

## TS2Vec
### Problem Definition
&emsp;&emsp;Given a set of time series $X=\{x_1,x_2,\dots,x_N\}$ of $N$ instances, our goal is to learn a nonlinear embedding function $f_\theta$ that maps each $x_i$ to its representation $r_i$ that best describes itself. The input time series $x_i$ has dimension $T\times F$, where $T$ is the sequence length and $F$ is the feature dimension. The representation $r_i=\{r_{i,1},r_{i,2},\dots r_{i,T}\}$ contains representation vector $r_{i,t}\in\mathbb{R}^K$ for each timestamp $t$, where $K$ is the dimension of representation vectors.

$$x_i\overset{f_\theta}{\longrightarrow}r_i=\{r_{i,1},r_{i,2},\dots r_{i,T}\}$$

### Architecture

![TS2Vec.png](https://i.loli.net/2021/08/13/kvEN7uX2mlQDMyb.png)

* Input projection layer is a fully connected layer that maps the observation $x_{i,t}$ at timestamp $t$ into a latent vector $z_{i,t}$
  * Notice that we add extra time features to the input, including multi-scale time features
* Masking layer masks $z_i=\{z_{i,t}\}$ with a binary mask $m\in\{0,1\}^T$ that is randomly generated from a Bernoulli distribution with $p=0.5$ along the time axis
  * In common practices raw values are masked for data augmentation
  * Notice that $0$ naturally exists in time series, thus we choose to mask the latent vector but not raw input
* Dilated CNN module is used to extract the representation for each timestamp which consists of ten residual blocks to capture hierarchical features

## Hierarchical Contrastive Loss

$$L_{temp}^{(i,t)}=-\log\frac{\exp(r_{i,t}\cdot\acute{r_{i,t}})}{\sum_{\acute{t}\in\Omega}[\exp(r_{i,t}\cdot\acute{r_{i,t}})+\mathbb{I}_{t\not=\acute{t}}\exp(r_{i,t}\cdot r_{i,\acute{t}})]}$$

$$L_{inst}^{(i,t)}=-\log\frac{\exp(r_{i,t}\cdot\acute{r_{i,t}})}{\sum_{j=1}^B[\exp(r_{i,t}\cdot\acute{r_{j,t}})+\mathbb{I}_{i\not=j}\exp(r_{i,t}\cdot r_{j,t})]}$$

$$L_{dual}=\sum_i\sum_t(L_{temp}^{(i,t)}+L_{inst}^{(i,t)})$$

* $r_{i,t}$ and $\acute{r_{i,t}}$ denote the representations for the same timestamp $t$ but from two
augmentations of $x_i$
* Temporal Contrastive Loss：negative samples are in the same time series at different timestamps
* Instance-wise Contrastive Loss：negative samples are in the mini-batch at the same timestamp

![TS2Vec_loss.png](https://i.loli.net/2021/08/13/dHtEXUuB1yfnvk2.png)

## 笔者注解
&emsp;&emsp;本文针对时间序列提出了可学习时间戳层级特征表示的统一框架：TS2Vec。TS2Vec 的多层级（hierarchical）体现在两个方面，一是编码器中基于残差结构提取的多尺度特征，类似于 U-net、FPN；二是基于对比损失选择不同的负采样方案学习时间序列的本质时序特征（inherent feature）和全局特征（Instance-wise feature）。论文末尾留下了两个疑问以供进一步的探索。

1) How to obtain cross-dataset representations effectively may be studied in future research?

2) In addition, we notice that our model may be vulnerable to adversarial attacks that fool the encoder to learn drifted representations.
