## 统计学习及监督学习概论
&emsp;&emsp;统计学习，一言蔽之，基于数据建立概率统计模型并对未知新数据进行预测和分析。
* 监督学习：建立输入 $X$ 到输出 $Y$ 的映射，两者遵循联合概率分布 $P(X, Y)$
  * 监督学习的应用主要在三个方面：**分类、标注和回归**
* 无监督学习：学习数据中的统计规律或潜在结构，可进行聚类、降维或概率估计
* 强化学习：基于马尔可夫决策过程的迭代模型
* 半监督、主动学习：降低标注成本

&emsp;&emsp;统计学习方法一般可分为两大类，贝叶斯学习和核方法。
* 贝叶斯学习：最大化给定数据条件下模型的条件概率，即后验概率
* 核方法：使用核函数表示和学习非线性模型

&emsp;&emsp;损失函数和风险函数用于刻画模型的准确性，其中
* 经验风险最小化：关注训练数据本身，容易产生过拟合
* 结构风险最小化：增加惩罚项（正则化项），遵循“奥卡姆剃刀”原则

![预测误差.jpg](https://i.loli.net/2020/07/17/ALvwrSjo7B56VRD.jpg)

&emsp;&emsp;我们常用精确率（precision）和召回率（recall）作为模型的评价指标。
* 精确率关注局部预测范围的准确率
* 召回率更关注整个数据

![TP_FP.jpg](https://i.loli.net/2020/01/31/NfJ1M4KHC7U8p5u.png)

$$P=\frac{TP}{TP+FP}\qquad R=\frac{TP}{TP+FN}\qquad \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$

## 感知机
&emsp;&emsp;感知机（perceptron）是二类分类的线性分类模型，输入**特征向量**后输出实例的类别。它的本质是输入空间中用于划分正负类的超平面（hyperplane）。

$$f(x)=\text{sgn}(\omega^Tx+b)=\begin{cases}+1& \omega^Tx+b\geq0 \\ -1& \omega^Tx+b<0\end{cases}$$

$$L(\omega,b)=-\sum_{x_i\in M}y_i(\omega^Tx_i+b)$$

&emsp;&emsp;损失函数的一个优秀选择是误分类点到超平面 $S$ 的总距离，其中 $M$ 为误分类点的集合，舍去了距离表达式中法向量的范数系数。这个损失函数就是感知机学习的经验风险函数。<br>
&emsp;&emsp;感知机的学习策略即在假设空间内选取使损失函数最小的模型参数 $\omega,b$，即感知机模型。