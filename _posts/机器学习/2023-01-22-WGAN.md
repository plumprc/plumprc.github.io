---
title: Wasserstein GAN (to do...)
date: 2022-12-04 19:25:54
categories:
- 机器学习
tags:
- 机器学习
- 表征学习
- 生成模型
---

|PAPER
|:-:
|<a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a>
|<a href="https://arxiv.org/abs/1701.04862">Towards Principled Methods for Training Generative Adversarial Networks</a>
|<a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a>

## Kullback–Leibler and Jensen–Shannon Divergence
&emsp;&emsp;KL-divergence and JS-divergence are generally used to measure the distance between two probability distribution $p$ and $q$. KL-divergence is formulated as follows.

$$KL(p\Vert q)=\int p(x)\log\frac{p(x)}{q(x)}\text{d}x$$

&emsp;&emsp;In practice, we usually assume that $p$ and $q$ follow the Gaussian for simplifying the calculation. Notably, KL-divergency is asymmetric, and thus weak $p$ may induce unsignificant results. JS-divergency balances $p$ and $q$ as:

$$JS(p\Vert q)=\frac{1}{2}KL(p\Vert \frac{p+q}{2})+\frac{1}{2}KL(q\Vert\frac{p+q}{2})$$

&emsp;&emsp;JS-divergency is symmetric and stable if switching $p$ and $q$. However, KL and JS-divergency both rely on a strong assumption that $p$ and $q$ should overlap.

## Generative Adversarial Network

## Wasserstein GAN