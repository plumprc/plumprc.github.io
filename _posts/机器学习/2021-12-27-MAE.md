---
title: Masked Autoencoders (MAE)
date: 2021-12-27 16:51:00
categories:
- 机器学习
tags:
- 机器学习
- 表示学习
- 自监督学习
---

<center>PAPER: <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners
</a></center>

## Motivations
&emsp;&emsp;What makes masked autoencoding different between vision and language?
* **Architecture gap**: It is hard to integrate tokens or positional embeddings into CNN, but ViT has addressed this problem.
* **Information density**: Languages are highly semantic and information-dense but images have heavy spatial redundancy, which means we can recover missing patches from neighboring patches with little high-level understanding of the data.
* **Decoder design**: In vision, the decoder reconstructs pixels, which means the output is of a lower semantic than  common recognition tasks. While in languages, the decoder predicts missing words that contain rich semantic information.

## Masked Autoencoders
&emsp;&emsp;Compared with traditional auxiliary tasks in vision, MAE introduces a strategy: masking a very **high portion** (75%) of random patches to induce the model to learn a holistic understanding. MAE masks random patches from the input image and reconstructs the missing patches **in the pixel space**.

![MAE_res.png](https://s2.loli.net/2021/12/27/8uGCzZoBEnDUqJx.png)

&emsp;&emsp;MAE has an asymmetric encoder-decoder architecture design as follows. Encoder operates only on the visible subset of patches (without mask tokens) and decoder is lightweight and reconstructs the input from the latent representation along with mask tokens.

![MAE.png](https://s2.loli.net/2021/12/27/m8t9o7KGRNvfrgY.png)

```python
patch_list = img2patch(image)
patch_vis = shuffle(patch_list)[:25]
latent = encoder(unshuffle(patch_vis), position_embeddings)
target = decoder(latent, mask)
```

* MAE add positional embeddings to all tokens in this full set.
* Mask tokens would have no information about their location in the image.

![MAE_exp.png](https://s2.loli.net/2021/12/27/73TxgFwlEIcnpsr.png)

&emsp;&emsp;Figure (a) and (b) shows that decoder could be more flexible and lighter to reduce computation. Figure (c) demonstrates that removing mask token from the encoder is better. Encoder could always see real patches but not mask patches to exploit meaningful information. Figure (d) tells us that tokenization is not necessary for MAE. Figure (e) and (f) shows that MAE could work with minimal or no augmentation. On the contrary, contrastive learning methods in vision require heavy data augmentation up to now. In MAE, the masks are different for each iteration and so they generate new training samples regardless of data augmentation.
