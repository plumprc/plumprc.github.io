---
title: CS231n学习笔记（一）
date: 2020-03-12 17:19:23
categories: 
- 计算机视觉
tags:
- 计算机视觉
- 卷积神经网络
---

|Leading
|:-:
|K-Nearest Neighbors
|Loss function
|Neural Network

# Scatter points
## K-Nearest Neighbors
&emsp;&emsp;Usually, we can use norm to depict the size feature of one matrix. So the distance between two matrix can be derived from norm :

$$L_1: d_1(I_1, I_2)=\sum_p|I_1^p-I_2^p|$$

$$L_2: d_2(I_1, I_2)=\sqrt{\sum_p(I_1^p-I_2^p)^2}$$

&emsp;&emsp;Based on L-distance, [K-Nearest Neighbors](http://vision.stanford.edu/teaching/cs231n-demos/knn/) is emerged. As its name says, take **majority vote** from K closest points to form several local area.

![cv1.png](https://i.loli.net/2020/03/12/TWaOyBSHFYQZqdA.png)

## Linear Classifier
![cv2.png](https://i.loli.net/2020/03/12/eBfKLbAEiRQ2Pak.png)

$$\underset{10\times1}{f(x, W)}=\underset{10\times3072}{W}\cdot\underset{3072\times10}{x}+\underset{10\times1}{b}$$

&emsp;&emsp;You can intuitionally see the embryo of FC and bias unit.

## Loss function
&emsp;&emsp;A loss function tells how good our current classifier is. Given a dataset of examples : $\displaystyle\{(x_i, y_i)\}_{i=1}^N$, the loss over the dataset is a average of loss over examples : 

$$L=\frac{1}{N}\sum_iL_i(f(x_i, W), y_i)$$

&emsp;&emsp;Design an excellent loss function and minimize it, then we can obtain a good weight $W$. **Multi class SVM** loss is used in many scene : 

$$s=f(x_i, W)\quad L_i=\sum_{j\ne y_i}\max(0, s_j-s_{y_i}+1)$$

&emsp;&emsp;Considering the coefficient of its weight $W$, $W$ has the same impact as $2W$. Thus, the best $W$ is not unique, so we propose a regularization (like penalty) :

![cv3.png](https://i.loli.net/2020/03/12/XwxArJ1LPh5jRfc.png)

## Gradient desent
&emsp;&emsp;The change of parameters can be observed by **perturbing** the system. We can alsoe update our parameters through gradient desent.

&emsp;&emsp;Stochastic Gradient Descent (SGD), using a **minibatch** of examples to compute the loss function, has better convergence.

## Neural Networks
&emsp;&emsp;There exists a problem is that Linear Classifiers are not very powerful. It can only learn one template per class and draw just linear decision boundaries. One solution is to do Feature Transformation, Transform data with a cleverly chosen feature transform f (color histogram, Histogram of Oriented Gradients, Bag of Words), then apply linear classifier.

&emsp;&emsp;Thus, neural network is used to extract non-linear feature. Here is a 2-layer Neural Network : $f=W_2\max(0, W_1x)$

![cv4.png](https://i.loli.net/2020/03/12/vGQBYyg3xdMhZ4K.png)

&emsp;&emsp;The function $max(0, z)$ is called the **ReLU (Rectified Linear Unit)**. More activation functions are show as follows. 

![cv5.png](https://i.loli.net/2020/03/12/lBjXE1phan9gQvq.png)

&emsp;&emsp;ReLU is a good default choice for most problems.

## Back propagation
&emsp;&emsp;Based on chain rule and graph computation, we can use BP to quickly get the gradient.
* forward : compute numerical value in node of the graph
* backward : compute the gradient

![cv6.png](https://i.loli.net/2020/03/12/jmZ4vxB9OYqQAp2.png)

&emsp;&emsp;Matrix derivation should multiply a Jacobian coefficient.

## Convolutional Neural Networks

![cv7.png](https://i.loli.net/2020/03/12/gIBzAe1HFsoCJcT.png)

* Fully Connected Layer: stretch to flat
* Convolutiona Layer: preserve spatial structure
  * filter: slide to collect the feature map
  * stride: the size filter sliding each step
  * padding: consider the margin
* Pooling Layer: makes the representations smaller and more manageable

![cv8.png](https://i.loli.net/2020/03/12/WdmKijPoIVublxv.png)

&emsp;&emsp;Notice 1 x 1 filter can reduce the number of channels, realize information integration across channels, and keep the scale of feature map unchanged.
