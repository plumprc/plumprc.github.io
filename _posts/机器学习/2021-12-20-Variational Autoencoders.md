---
title: Variational Autoencoders
date: 2021-12-20 16:06:00
categories:
- 机器学习
tags:
- 机器学习
- 表示学习
---

<center>PAPER: <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></center>

## Motivations
&emsp;&emsp;In generative modeling, we often need to get the distribution $p(x)$ of the data $x$. That is to say, if the $x$ looks like the real data, it should get high probability. If the $x$ looks like the random noise, it should get low probability. Formally speaking, real data $x$ is sampled from an unknown distribution $p_t(x)$ and our goal is to learn a model $p$ which we can sample from, such that $p$ is as similar as possible to $p_t$.

<div align="center"><img src="https://s2.loli.net/2021/12/19/uQjUxO7hB9RTDW5.png" width="75%"/></div>

## Latent Variable Models
&emsp;&emsp;Considering the high dimension of the raw data $x$, we should embed $x$ into latent space $z$ which can easily sampled from $p(z)$. Assume we have a family of functions $f(z;\theta)$, where $\theta$ is learnable parameters. Optimize $\theta$ such that $f(z;\theta)$ can produce samples like $x$ with high probability.

$$\arg\max_\theta p(x)=\int p(x\vert z;\theta)\cdot p(z)\text{d}z$$

&emsp;&emsp;Here, $f(z;\theta)$ is replaced by $p(x\vert z;\theta)$ due to maximum likelihood. In VAEs, the choice of this output distribution is often Gaussian. You can also use other distribution but you should guarantee that $\theta$ is continuous.

## Variational Autoencoders
&emsp;&emsp;To optimize $p(x)$, there are two problems we should deal with:
* How to define the latent variables $z$?
* How to deal with the integral over $z$?

&emsp;&emsp;VAEs assumes that there is no simple interpretation of the dimensions of $z$ and instead assert that samples of $z$ can be drawn from a simple distribution. In fact, any distribution in $d$ dimensions can be generated by taking $d$ variables which are normally distributed. Thus, we can choose $p(z)=N(0,1)$. Then what we should do is to sample a large number of $z$ values and compute $p(x)\approx\displaystyle\frac{1}{n}\sum_ip(x\vert z_i)$. The problem is that in high dimensional space, this way needs a large amount of samples. Another problem is that if $p(x\vert z)$ is an isotropic Gaussian, optimizing $p(x)$ is equal to minimize the squared Euclidean distance between $f(z)$ and $x$, which is not a good **similarity metrics** in practice.

&emsp;&emsp;In practice, for most $z$, $p(x\vert z)$ will be nearly zero. The key idea behind VAEs is to attempt to sample values of $z$ that are likely to have produced $x$. This means we need a new function $q(z\vert x)$ where we can get a distribution over $z$ values which are likely to produce $x$. The space of $z$ under $q$ is much smaller than $z$ under $p(z)$. Then we should make $q(z\vert x)$ and the true posterior distribution $p(z\vert x)$ more similar so that we can use $p(x)\approx\mathbb{E}_{z\sim q}p(x\vert z)$ to estimate $p(x)$. Consider the KL divergence (denoted as $D$) between them:

$$D(q(z\vert x),p(z\vert x))=\mathbb{E}_{z\sim q}[\log q(z\vert x)-\log p(z\vert x)]$$

$$\log p(z\vert x)=\log p(x\vert z)+\log p(z)-\log p(x)$$

$$\log p(x)-D(q(z\vert x),p(z\vert x))=\mathbb{E}_{z\sim q}\log p(x\vert z)-D(q(z\vert x),p(z))$$

&emsp;&emsp;Our goal is to maximize $p(x)$ and minimize $D(q(z\vert x),p(z\vert x))$, which is equal to optimize the right hand side of the equation.
* maximize the expectation of the reconstruction of data points from the latent vector
* minimize the divergence between the estimated latent vector and the true latent vector

&emsp;&emsp;To apply SGD on the right hand side of above equation, we should specify all the terms. We know $p(z)=N(0,1)$ and $q$ is often initialized as Gaussian with learnable mean and variance. The expectation $\mathbb{E}_{z\sim q}\log p(x\vert z)$ can be estimated by [reparameterization trick]().

![VAE.png](https://s2.loli.net/2021/12/16/IrnsQz2dLAb47w8.png)

$$\nabla_\theta\mathbb{E}_{q(z;\theta)}\log p(x\vert z)=\mathbb{E}_{\varepsilon\sim N(0,1)}\nabla_\theta\log p(x\vert f(\varepsilon;\theta))$$

&emsp;&emsp;Here is a demo VAE trained on MNIST:

```python
class VAE(nn.Module):
    def __init__(self, image_size=784, h_dim=400, z_dim=20):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(image_size, h_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(h_dim, z_dim*2)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(z_dim, h_dim),
            nn.ReLU(),
            nn.Linear(h_dim, image_size),
            nn.Sigmoid()
        )
    
    def reparameterize(self, mu, logvar):
        std = logvar.mul(0.5).exp_().to(device)
        esp = torch.randn(*mu.size()).to(device)
        z = mu + std * esp
        return z
    
    def forward(self, x):
        # x: [batch, 784]
        h = self.encoder(x)
        # encoder: [batch, 40]
        mu, logvar = torch.chunk(h, 2, dim=1)
        # paras, z: [batch, 20]
        z = self.reparameterize(mu, logvar)
        return self.decoder(z), mu, logvar


def loss_fn(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')

    # see Appendix B from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())
    return BCE + KLD
```

![recon_image.png](https://s2.loli.net/2021/12/19/Dbr3H4vpC1ywh9Q.png)
