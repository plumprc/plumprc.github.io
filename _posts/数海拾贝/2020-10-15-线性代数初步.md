---
title: 线性代数初步（持续更新中）
date: 2020-10-15 17:27:05
categories: 
- 数学
tags:
- 数学
- 线性代数
---

![linear algebra.png](https://i.loli.net/2020/11/23/NyvlpicPC9esuBn.png)

# 矩阵（matrix）
## 向量空间（vector space）
&emsp;&emsp;向量（vector）是标量的有序组合。如位置向量 $\pmb{x}=[2,3,5]^T$，再如 IRIS 数据集中由花萼长度，花萼宽度，花瓣长度，花瓣宽度组成的向量 $\pmb{v}=[L_1,W_1,L_2,W_2]^T$。向量与欧氏空间内的点（point）一一对应，其丰富的几何性质赋予了向量简洁的运算规则，这就是引入向量的根本原因。

&emsp;&emsp;在欧式空间的视角下，向量可以看作是多个基底（basis）的加权组合，我们可以用左乘矩阵来表述这种关系：

$$
\begin{bmatrix}
   3 \\ 2
\end{bmatrix}
=3
\begin{bmatrix}
   1 \\ 0
\end{bmatrix}
+2
\begin{bmatrix}
   0 \\ 1
\end{bmatrix}
=
\begin{bmatrix}
   1 & 0 \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   3 \\ 2
\end{bmatrix}
$$

&emsp;&emsp;考虑向量 $\pmb{b_1}=[1,0]^T$，$\pmb{b_2}=[1,1]^T$ ，以 $B=[\pmb{b_1}, \pmb{b_2}]$ 为新基底，很容易得出原始坐标 $(3,2)$ 在新基下的坐标为 $(1,2)$。事实上这种对应关系亦可以扩展至整个 $\mathbb{R}^2$ 平面。

![坐标变换.png](https://i.loli.net/2020/10/15/ZIy82uP5ifLe3Dj.png) TODO: 缩小图片

$$
\begin{bmatrix}
   1 & 0 \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   3 \\ 2
\end{bmatrix}
=
\begin{bmatrix}
   1 & 1 \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   1 \\ 2
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
   1 & 0 \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   x+y \\ y
\end{bmatrix}
=
\begin{bmatrix}
   1 & 1 \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   x \\ y
\end{bmatrix}
$$

&emsp;&emsp;可以看出，欧式空间下的同一点，在不同基的表示下会得出不同的坐标，据此我们可以得出欧氏空间内的坐标变换公式： $\pmb{x}=P_B\pmb{x_B}$。同时我们也揭示了左乘矩阵的本质，即**换基，显式的建立欧氏空间内不同基表示的点的坐标间的映射关系**。在计算机图形学领域内，我们能更加直观的理解矩阵所表示的这种映射关系，如剪切变换（shearing）、旋转变换（rotation）等。

![旋转和剪切变换.png](https://i.loli.net/2020/10/15/wWO6HBvLYcstr9V.png)

&emsp;&emsp;因此矩阵，**其实质就是针对向量进行的线性变换**。考虑变换 $T(\pmb{x})=A_{m\times n}\pmb{x}$，该变换以 $A$ 的列向量为基底建立了 $\mathbb{R}^n\rightarrow\mathbb{R}^m$ 的映射关系。这种变换与函数类似，均具有线性（$f+g$）和可复合性（$f\circ g$），这也是矩阵加法和乘法的意义所在。

## 内积（dot product）
&emsp;&emsp;我们已经知道，向量与欧氏空间内的点一一对应，与之相伴向量也被赋予了更多几何上的性质。为了更好的度量向量的几何性质（诸如距离、角度等），我们提出了内积（dot product）的概念。考虑 $\mathbb{R}^n$ 中的向量 $\pmb{a}$ 和 $\pmb{b}$，它们的内积被简单定义为：$\pmb{a}\cdot\pmb{b}=\pmb{b}^T\pmb{a}$。内积可表述的相关几何性质如下表所示。

|内积表示|几何含义
|:-:|:-:
|$\parallel\pmb{a}\parallel=\pmb{a}\cdot\pmb{a}$|向量长度
|$\pmb{a}\cdot\pmb{b}=0$|向量垂直（正交）
|$\cos\theta=\displaystyle\frac{\pmb{a}\cdot\pmb{b}}{\parallel\pmb{a}\parallel\parallel\pmb{b}\parallel}$|向量夹角

&emsp;&emsp;进一步，我们将空间 $V$ 与 $W$ 的正交性解释为：$V$ 内的任意向量与 $W$ 内的任意向量内积均为零，$W$ 可称为 $V$ 的正交补空间（记为 $V^\bot$）

&emsp;&emsp;以上属于向量空间内的内积，在不同基的视角下内积亦可推广至其他空间。例如函数空间内的内积 $\langle f,g\rangle=\int_a^bf(t)g(t)\rm{d}t$，多项式空间内基于内积生成的正交多项式等。

## 线性方程组（linear equations）
&emsp;&emsp;本科阶段的学习大多从线性方程组的解来引出矩阵乘法的定义，从解析式上看线性方程组的解与矩阵方程的解无疑是等价的。

$$
\begin{cases}
   x-2z=1 \\ 
   x+2y=5 \\
   y+z=2
\end{cases}
\Rightarrow
\begin{bmatrix}
   1 & 0 & -2 \\
   1 & 2 & 5 \\
   0 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
   x \\ y \\ z
\end{bmatrix}
=
\begin{bmatrix}
   1 \\ 5 \\ 2
\end{bmatrix}
$$

&emsp;&emsp;考虑右侧的矩阵方程 $A\pmb{x}=\pmb{b}$，它表示以 $A$ 的列向量为基底对 $\pmb{x}$ 进行线性变换得到 $\pmb{b}$。记 $A=[\pmb{c}_1,\pmb{c}_2]$，为方便表述我们称由矩阵列向量张成的子空间为列空间（column space），即满足 $\pmb{y}=A\pmb{x}$ 的 $\pmb{y}$ 的集合。方程可解即表示为 $b\in\text{span}\{\pmb{c_1},\pmb{c_2}\}$，即 $\pmb{b}\in C(A)$。方程的解可以通过高斯消元法对增广矩阵（augmented bmatrix）进行初等行变换得到。

$$
\left[\begin{array}{ccc|c}
   1 & 0 & -2 & 1\\
   1 & 2 & 5 & 5\\
   0 & 1 & 1 & 2\\
\end{array}\right]\rightarrow
\left[\begin{array}{ccc|c}
   1 & 0 & -2 & 1\\
   0 & 1 & 1 & 2\\
   0 & 0 & 0 & 0\\
\end{array}\right]
$$

$$
\pmb{x}=
\begin{bmatrix}
   1 \\ 2 \\ 0
\end{bmatrix}+
k\begin{bmatrix}
   2 \\ -1 \\ 0
\end{bmatrix},k\in\mathbb{R}
$$

&emsp;&emsp;再考虑齐次方程 $A\pmb{x}=\pmb{0}$，显然行变换不会影响等式右侧的零，最终得到的解为 $\pmb{x}=[2k,-k,0]^T$，与 $A\pmb{x}=\pmb{b}$ 的解集相比仅仅差了一个常向量（constant vector）。我们称所有满足 $A\pmb{x}=\pmb{0}$ 的 $\pmb{x}$ 的集合为 $A$ 的零空间（null space），$A\pmb{x}=\pmb{b}$ 的解空间可看作是零空间的整体平移，两个空间存在一个恒等的偏差（bias）。

&emsp;&emsp;对于 $A$ 进行初等行变换得到的上三角矩阵 $U_{m\times n}$，称对角线上的非零元为主元（pivot），称零元为自由变量（free variables），我们很容易就能找到一个关于秩和零空间的著名定理：秩-零度定理（rank-nullity theorem）

$$\text{rank}A+N(A)=\#\text{pivot}+\#\text{free variables}=m$$

&emsp;&emsp;以上，我们通过线性方程组引入了矩阵的两个重要性质：列空间与零空间。将 $A$ 改写为 $[r_1,r_2,r_3]^T$，对转置后的矩阵引入行空间（row space）与左零空间（column space of A transpose）的概念，敏锐的读者很快就能发现，矩阵的行空间与零空间事实上是正交的（orthogonal）。

$$
A\pmb{x}=\pmb{0}\rightarrow
\begin{bmatrix}
   \pmb{r_1} \\ \pmb{r_2} \\ \pmb{r_3}
\end{bmatrix}\pmb{x}=
\begin{bmatrix}
   \pmb{r_1\cdot x} \\ \pmb{r_2\cdot x} \\ \pmb{r_3\cdot x}
\end{bmatrix}=
\begin{bmatrix}
   0 \\ 0 \\ 0
\end{bmatrix}
$$

&emsp;&emsp;由于行空间与零空间的正交互补性，结合秩-零度定理，我们可以得到关于秩的一个经典结论：**行秩等于列秩**。

![四个基本子空间.jpg](https://i.loli.net/2020/10/19/tGgfES3FpweQbD4.png)

|基本子空间|相关性质
|:-:|:-:
|列空间 $C(A)$|$\dim C(A)=r=\#\text{pivot}$
|零空间 $N(A)$|$\dim N(A)=m-r=\text{\#free variables}$
|行空间 $C(A^T)$|$\dim C(A^T)=r$
|左零空间 $N(A^T)$|$\dim{N(A^T)}=n-r$

&emsp;&emsp;一言蔽之，矩阵 $A_{m\times n}$ 描述了线性变换 $T:\mathbb{R}^m\rightarrow\mathbb{R}^n$ ，其蕴含的四个基本子空间囊括了矩阵变换的本质特征。

# 变换（transformation）
## 线性变换（linear transformation）
&emsp;&emsp;考虑空间 $V:\mathbb{R}^n$ 与 $W:\mathbb{R}^m$，选取 $V$ 上的一组基 $B={\pmb{b}_1,\pmb{b}_2,\dots,\pmb{b}_n}$，对 $V$ 内任意向量 $\pmb{x}$，我们有 $\pmb{x}=r_1\pmb{b}_1+\dots+r_n\pmb{b}_n$。记 $\pmb{x}_B=[r_1,\dots,r_n]^T$，即 $\pmb{x}$ 在基 $B$ 下的坐标。我们知道，矩阵 $A_{m\times n}$ 描述了线性变换 $T:\mathbb{R}^n\rightarrow\mathbb{R}^m$，则矩阵对向量的作用可表示为：

$$T(\pmb{x})=r_1T(\pmb{b}_1)+\dots+r_nT(\pmb{b}_n)=M\pmb{x}_B$$

&emsp;&emsp;其中 $M=[T(\pmb{b}_1),\dots,T(\pmb{b}_n)]$。可以见得，空间到空间的线性变换，可以借由空间内的一组基来显式表示，这一点在欧式空间内的坐标变换公式 $\pmb{x}=P_B\pmb{x}_B$ 上体现的尤为明显。对于变换后的 $T(\pmb{x})$，可以取 $W$ 内的一组基 $C$ 来表示，即 $T(\pmb{x})_C=M\pmb{x}_B$。

![线性变换.jpg](https://i.loli.net/2020/10/27/JdzE5mbXwtxeVGs.jpg)

&emsp;&emsp;我们记 $\text{Im}T$ 为经过变换 $T$ 后的像（image），$\ker T$ 是像为零元的子空间（kernel，即零空间），如此即可以从线性变换的视角来理解秩-零度定理。

![Rank-nullity.png](https://i.loli.net/2020/10/20/eDFSA6MU1JOY7mZ.png)

&emsp;&emsp;再例如多项式上的微分算子事实上也是一种线性变换。取多项式空间上的一组基 $B={1,x,x^2,\dots,x^n}$，则微分算子可以表示为： 

$$D=[\frac{\partial1}{\partial x}_B,\dots,\frac{\partial x^n}{\partial x}_B]=
\begin{bmatrix}
   0 & 1 & 0 &  \cdots & 0\\
   0 & 0 & 2  & \cdots & 0 \\
   0 & 0 & 0  & \ddots & \vdots \\
   \vdots & \vdots & \vdots & \ddots & n-1 \\
   0 & 0 & 0 & \cdots & 0
\end{bmatrix}
$$

&emsp;&emsp;综上所述，同一个线性变换，可以用不同的基来显式表示，事实上这些基之间的关系互为同构（isomorphism）。

## 投影（projection）
TODO: 这一段需要好好修饰一下<br>
&emsp;&emsp;考虑向量 $\pmb{a}$ 和 $\pmb{b}$，向量 $\hat{\pmb{b}}$ 满足 $\hat{\pmb{b}}-\pmb{b}$ 与 $\pmb{a}$ 正交，这时候我们认为 $\hat{\pmb{b}}$ 是 $\text{span}\{\pmb{a}\}$ 上与 $\pmb{b}$ 最接近的向量，即 $\pmb{b}$ 在 $\pmb{a}$ 上的投影（projection）。

![projection.png](https://i.loli.net/2020/10/30/8GMtq7gBzomKlXi.png)

&emsp;&emsp;记 $\hat{\pmb{b}}=\lambda\pmb{a}$，我们有：

$$\pmb{a}^T(\pmb{b}-\lambda\pmb{a})=0\Rightarrow\lambda=\frac{\pmb{a}^T\pmb{b}}{\pmb{a}^T\pmb{a}}$$

$$\therefore\hat{\pmb{b}}=\lambda\pmb{a}=\pmb{a}\lambda=\pmb{a}\frac{\pmb{a}^T\pmb{b}}{\pmb{a}^T\pmb{a}}=\frac{\pmb{a}\pmb{a}^T}{\pmb{a}^T\pmb{a}}\pmb{b}=P_a\pmb{b}$$

&emsp;&emsp;如上，我们得到了向量 $\pmb{a}$ 上的投影矩阵 $P_a$，其列空间即 $\pmb{a}$ 所在直线。

### 最小二乘法（least squared）
（如何引入最小二乘法。。。）

&emsp;&emsp;考虑矩阵方程 $A\pmb{x}=\pmb{b}$，方程无解即 $\pmb{b}\notin C(A)$。根据上文所阐述的思想，我们需要在 $A$ 的列空间内寻找 $\hat{\pmb{b}}=A\hat{\pmb{x}}$，使得 $\pmb{b}$ 与 $\hat{\pmb{b}}$ 尽量接近，即：

$$\hat{\pmb{x}}=\argmin(\parallel\pmb{b}-A\hat{\pmb{x}}\parallel)$$

&emsp;&emsp;当 $\pmb{b}$ 和 $\hat{\pmb{b}}$ 最接近时，$\hat{\pmb{b}}$ 即 $\pmb{b}$ 在 $A$ 列空间上的投影。考虑到矩阵的列空间与左零空间互为正交补空间，因此对 $\pmb{b}$ 做正交分解 $\pmb{b}=\hat{\pmb{b}}+\pmb{e}$，必定有 $\pmb{e}\in N(A^T)$，因此：

![least_squared.png](https://i.loli.net/2020/10/30/z4otDeumIO1wBdQ.png)

$$A^T(\pmb{b}-A\hat{\pmb{x}})=\pmb{0}\Rightarrow\hat{\pmb{x}
}=(A^TA)^{-1}A^T\pmb{b}$$

$$\therefore\hat{\pmb{b}}=A(A^TA)^{-1}A^T\pmb{b}=P_A\pmb{b}$$

&emsp;&emsp;因此矩阵方程 $A^TA\hat{\pmb{x}}=A^T\pmb{b}$ 的解就是我们需要的解，它告诉我们如何寻找 $\pmb{b}$ 在矩阵列空间内的投影。同时我们也找到了关于列空间的投影矩阵 $P_A=A(A^TA)^{-1}A^T$。

&emsp;&emsp;例如，对于点 $(1,1)$，$(2,2)$，$(3,2)$，拟合直线 $y=C+Dt$ 使之与假想过三点的直线最为接近，对此我们有：

$$
\begin{cases}
   C+D=1 \\ 
   C+2D=2 \\
   C+3D=2
\end{cases}\Rightarrow
\begin{bmatrix}
   1 & 1 \\
   1 & 2 \\
   1 & 3
\end{bmatrix}
\begin{bmatrix}
   C \\ D
\end{bmatrix}=
\begin{bmatrix}
   1 \\ 2 \\ 3
\end{bmatrix}\Rightarrow A\pmb{x}=\pmb{b}
$$

$$
\therefore A^TA\hat{\pmb{x}}=A^T\pmb{b}\Rightarrow
\begin{bmatrix}
   3 & 6 \\
   6 & 14
\end{bmatrix}
\begin{bmatrix}
   C \\ D
\end{bmatrix}=
\begin{bmatrix}
   5 \\ 11
\end{bmatrix}\Rightarrow
\begin{bmatrix}
   C \\ D
\end{bmatrix}=
\begin{bmatrix}
   2/3 \\ 1/2
\end{bmatrix}
$$

&emsp;&emsp;最终我们得到了拟合后的直线 $y=\displaystyle\frac{2}{3}+\displaystyle\frac{1}{2}t$。事实上直接考虑 $\min(\parallel\pmb{b}-A\hat{\pmb{x}})$，通过拉格朗日乘数法求解也能得到相同的结果，只是计算量要更为庞大。

![曲线拟合示例.png](https://i.loli.net/2020/10/30/qExTDL1QNg7iSW4.png)

&emsp;&emsp;关于 $A^TA$，事实上其与 $A$ 拥有相同的零空间，且两者的秩是相等的，有兴趣的读者可以自行证明。

### 正交基
&emsp;&emsp;我们把两两相互正交的向量集称为正交集（orthogonal set）。容易证明，正交集是线性无关集，由于正交基的内积特性，选取正交基进行相关计算会十分的便利。

列基是单位正交基的矩阵有很多优良的计算性质（保长度、正交）
* $\parallel U\pmb{x}\parallel=\parallel\pmb{x}\parallel$
* $(U\pmb{x})\cdot(U\pmb{y})=\pmb{x}\cdot\pmb{y}$
* $(U\pmb{x})\cdot(U\pmb{y})=0\iff\pmb{x}\cdot\pmb{y}=0$
* $U^TU=I$

&emsp;&emsp;格拉姆-施密特法（Graham-Schmidt）可以很快的构造出正交基，它的核心思想是基于投影的正交分解。考虑由一组线性无关的向量组成的基 $B=\{\pmb{a},\pmb{b},\pmb{c}\}$，选取 $\pmb{q}_1=a$ 作为初始向量，对其他向量依次做正交分解即可快速生成新的正交基 $Q=\{\pmb{q}_1,\pmb{q}_2,\pmb{q}_3\}$。

$$\pmb{q}_2=\pmb{b}-P_a\pmb{b}=\pmb{b}-\frac{\pmb{a}\pmb{a}^T}{\pmb{a}^T\pmb{a}}\pmb{b}=\pmb{b}-\frac{\pmb{a}^T\pmb{b}}{\pmb{a}^T\pmb{a}}\pmb{a}$$

$$\pmb{q}_3=\pmb{c}-P_a\pmb{c}-P_b\pmb{c}=\pmb{c}-\frac{\pmb{a}^T\pmb{c}}{\pmb{a}^T\pmb{a}}\pmb{a}-\frac{\pmb{b}^T\pmb{c}}{\pmb{b}^T\pmb{b}}\pmb{b}$$

## 仿射变换（affine transformation）
### 仿射组合

### 齐次坐标（Homogeneous coordinates）
&emsp;&emsp;在 SLAM 中一个最基本的应用便是如何实现坐标平移。由于单个矩阵严格意义上只能实现拉伸或旋转，无法通过复合（composition）的方式实现整个平面的平移（原点位置无法改变），因此 SLAM 中引入了齐次坐标的概念，通过扩维实现某种意义上的“坐标平移”。（事实上剪切变换就实现了二维中单个坐标的平移）

$$
\begin{bmatrix}
   R & T \\
   0 & 1
\end{bmatrix}
\begin{bmatrix}
   x \\ y \\ 1
\end{bmatrix}
=
\begin{bmatrix}
   1 & 0 & h \\
   0 & 1 & k \\
   0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
   x \\ y \\ 1
\end{bmatrix}
=
\begin{bmatrix}
   x+h \\ y+k \\ 1
\end{bmatrix}
$$

### 超平面（Hyperplane）

# 分解（decomposition）
## 特征值分解（eigen value decomposition）
### 特征向量
&emsp;&emsp;在欧氏空间的视角下，矩阵变换可视作对空间内所有的点进行拉伸和旋转，而对于复合后的矩阵（如矩阵的幂），我们很难借助图像来表征这样的复合变换。

&emsp;&emsp;但假设存在这样的向量，矩阵对其只进行拉伸变换，我们就可以用拉伸系数的幂来表示矩阵的复合变换。我们称满足 $A\pmb{x}=\lambda\pmb{x}$ 的非零向量 $\pmb{x}$ 为特征向量（eigen vector），拉伸系数 $\lambda$ 为特征值（eigen value）；关于如何求解特征向量及特征值，在此不作赘述。将特征向量写成如下格式，我们很快就能得到关于特征向量的一个重要结论：**对角化定理**。

$$
\begin{cases}
   A\pmb{x}_1=\lambda_1\pmb{x}_1 \\ 
   A\pmb{x}_2=\lambda_2\pmb{x}_2 \\
   A\pmb{x}_3=\lambda_3\pmb{x}_3
\end{cases}\Rightarrow
A\begin{bmatrix}
   \pmb{x}_1 & \pmb{x}_2 & \pmb{x}_3
\end{bmatrix}=
\begin{bmatrix}
   \pmb{x}_1 & \pmb{x}_2 & \pmb{x}_3
\end{bmatrix}
\begin{bmatrix}
   \lambda_1 & 0 &  \cdots & 0\\
   0 & \lambda_2  & \cdots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & 0 & \lambda_n
\end{bmatrix}
$$

&emsp;&emsp;容易证明，不同特征值对应的特征向量间线性无关（一个方向只有一个拉伸系数），因此由特征向量组成的矩阵 $P$ 一定是可逆的，因此有 $A=PDP^{-1}$，其中 $D=\text{diag}\{\lambda_1,\dots,\lambda_n\}$，这就是矩阵的**特征值分解**。

&emsp;&emsp;我们已经知道，矩阵对应的线性变换可以借助不同的基来显式表示（通常由列向量组成的基表示），若选取特征向量组成的基 $B=\{\pmb{x}_1,\dots,\pmb{x}_n\}$，结合坐标变换 $\pmb{x}=P\pmb{x}_B$，我们有：

$$
\begin{aligned}
T_B &= [T(\pmb{x}_1)_B,T(\pmb{x}_2)_B,\dots,T(\pmb{x}_n)_B] \\
  &= [(\lambda_1\pmb{x}_1)_B,(\lambda_2\pmb{x}_2)_B,\dots,(\lambda_n\pmb{x}_n)_B] \\
  &= [P^{-1}A\pmb{x}_1,P^{-1}A\pmb{x}_2,\dots,P^{-1}A\pmb{x}_n] \\
  &= P^{-1}AP=D
\end{aligned}
$$

&emsp;&emsp;如上，我们知晓 $\pmb{x}\mapsto A\pmb{x}$ 和 $\pmb{u}\mapsto D\pmb{u}$ 事实上是等价的，仅仅是同一个线性变换的不同基的表示形式。我们称满足 $A=PBP^{-1}$ 的矩阵 $A,B$ 互为相似矩阵（similar bmatrix）。同时我们也会发现，使用特征向量作为基来描述矩阵表示的线性变换，在计算上会带来极大的便利。

### 动力系统
&emsp;&emsp;考虑如下应用场景：若每年要统计一个城市及其郊区的人口分布问题，如 $x_0=[0.60,0.40]^T$ 表示初始有 $60\%$ 的人口住在城市，$40\%$ 的人口住在郊区。假设每年有 $5\%$ 的城市人口流动到郊区，有 $3\%$ 的人口流动到城市，我们可以用矩阵方程来表征上述一阶差分动力系统：

$$\pmb{x}_{k+1}=M\pmb{x}_k=
\begin{bmatrix}
   0.95 & 0.03 \\
   0.05 & 0.97
\end{bmatrix}^{k+1}
\begin{bmatrix}
   0.60 \\
   0.40
\end{bmatrix}
,\quad k=0,1,2\dots$$

&emsp;&emsp;令 $\det(A-\lambda I)=0$，可以求出 $M$ 的特征值 $\lambda_1=1$，$\lambda_2=0.92$，回代即可求得其对应的特征向量 $\pmb{v}_1=[3,5]^T$，$\pmb{v}_2=[1,-1]^T$。以特征向量为基底表示初始向量，我们就可以很快求出矩阵的幂对原始向量的影响：

$$\pmb{x}_1=A\pmb{x}_0=c_1\lambda_1\pmb{v}_1+c_2\lambda_2\pmb{v}_2=0.125\pmb{v}_1+0.225\cdot(0.92)^k\pmb{v}_2$$

$$\lim_{k\rightarrow\infty}\pmb{x}_k=0.125\pmb{v}_1=\begin{bmatrix}
   0.375 \\
   0.625
\end{bmatrix}$$

&emsp;&emsp;如上，利用矩阵的特征值分解我们可以快速求出矩阵的幂对向量的影响。事实上这是一个由一阶差分方程刻画的经典的马尔可夫链（Markov chain），该链经长期行为后最终收敛至稳态（steady state）。

&emsp;&emsp;对于类似的一阶差分方程，我们亦可以借助点的轨迹来模拟动力系统的变化情况。下图模拟了三类经典的动力系统所代表的点的轨迹。

$$
A=\begin{bmatrix}
   0.8 & 0 \\
   0 & 0.64
\end{bmatrix}\quad
\pmb{x}_0=
\begin{bmatrix}
   \pm0.2 \\
   \pm0.3
\end{bmatrix}\quad
\text{吸引子（Attractor）}
$$

![track1.png](https://i.loli.net/2020/10/28/xe8wcEBKOdnstji.png)

$$
A=\begin{bmatrix}
   1.44 & 0 \\
   0 & 1.2
\end{bmatrix}\quad
\pmb{x}_0=
\begin{bmatrix}
   \pm0.002 \\
   \pm0.0005
\end{bmatrix}\quad
\text{排斥子（Repellent）}
$$

![track2.png](https://i.loli.net/2020/10/28/EUuS4GkwKtMZo6I.png)

$$
A=\begin{bmatrix}
   2 & 0 \\
   0 & 0.5
\end{bmatrix}\quad
\pmb{x}_0=
\begin{bmatrix}
   \pm0.01 \\
   \pm1
\end{bmatrix}\quad
\text{鞍点（Saddle point）}
$$

![track3.png](https://i.loli.net/2020/10/28/wupzdIJBxorqcR1.png)

### 复特征向量
&emsp;&emsp;遗憾的是，并不是所有的矩阵都有完备的实特征向量组，考虑二阶旋转阵 $R_2$，显然在 $R_2$ 作用下二维平面内不存在仅被拉伸的实向量。但如果将特征向量的概念从实数域扩展至复数域，我们很容易就能得到 $R_2$ 的两个复特征值 $i$ 和 $-i$，这与我们对复数 $i$ 表示旋转的认知是一致的。

$$\det(R_2-\lambda I)=0\Rightarrow
\begin{cases}
   \pmb{x}=\begin{bmatrix} \pm i \\ 1 \end{bmatrix} \\
   \lambda=\pm i
\end{cases}$$

![]() TODO: track it

&emsp;&emsp;考虑方阵 $A_{n\times n}$，由代数基本定理我们知道该阵的特征方程 $\det(A-\lambda I)$ 必有 $n$ 个复根（包含重根），因此在复数域内矩阵的性质可由矩阵的复特征向量完全表述。同时，对实矩阵 $A_{n\times n}$ 有 $\overline{A\pmb{x}}=A\overline{\pmb{x}}$，因此实矩阵的特征值和特征向量总是共轭成对出现的。

&emsp;&emsp;我们已经知道矩阵描述了一种旋转或拉伸的线性变换，事实上在复数域我们可以借助 $i$ 实现矩阵的旋转-拉伸分解。考虑以下二阶阵 $C$，它的特征值 $\lambda=a\pm bi$，记 $r=|\lambda|$，我们有：

$$C=\begin{bmatrix}
   a & -b \\
   b & a
\end{bmatrix}=\begin{bmatrix}
   r & 0 \\
   0 & r
\end{bmatrix}\begin{bmatrix}
   \cos\phi & -\sin\phi \\
   \sin\phi & \cos\phi
\end{bmatrix}
$$

&emsp;&emsp;其中 $\phi$ 是 $\lambda$ 的幅角。如上任意一个满足 $C$ 格式的矩阵都可以分解为一个表示拉伸的对角阵和旋转阵的乘积。更一般的，对于所有实二阶阵 $A_{2\times 2}$，记 $\lambda=a-bi$，$A\pmb{v}=\lambda\pmb{v}$ 我们有：

$$\begin{cases}
   A(\text{Re}\pmb{v})=a\text{Re}\pmb{v}+b\text{Im}\pmb{v} \\
   A(\text{Im}\pmb{v})=-b\text{Re}\pmb{v}+a\text{Im}\pmb{v}
\end{cases}\Rightarrow
A\begin{bmatrix}
   \text{Re}\pmb{v} & \text{Im}\pmb{v}
\end{bmatrix}=
\begin{bmatrix}
   \text{Re}\pmb{v} & \text{Im}\pmb{v}
\end{bmatrix}
\begin{bmatrix}
   a & -b \\
   b & a
\end{bmatrix}
$$

&emsp;&emsp;记 $P=[\text{Re}\pmb{v}, \text{Im}\pmb{v}]$，我们有 $AP=PC$，即 $A=PCP^{-1}$，此谓二阶矩阵的旋转-拉伸分解（RSD），对于更高阶矩阵亦有同样的分解方式，在此不做赘述。

### 对称矩阵（symmetric matrix）
&emsp;&emsp;考虑 $A^T=A$ 和复向量 $\pmb{x}$，我们有：

$$q=\overline{\pmb{x}}^TA\pmb{x}=(\overline{\pmb{x}}^TA\pmb{x})^T=\pmb{x}^TA\overline{\pmb{x}}=\overline{q}$$

&emsp;&emsp;显然 $q$ 是实数，考虑 $A\pmb{x}=\lambda\pmb{x}$，我们有 $\overline{\pmb{x}}^TA\pmb{x}=\lambda\overline{\pmb{x}}^T\pmb{x}$，因此对称矩阵的特征值必为实数，对于对称矩阵 $A_{n\times n}$，其必有 $n$ 个实特征值，必可被正交对角化为 $QDQ^T$。

$$A=QDQ^T=\lambda_1\pmb{q}_1\pmb{q}_1^T+\dots+\lambda_n\pmb{q}_n\pmb{q}_n^T$$

&emsp;&emsp;后式被称为对称矩阵的谱分解（spectral theorem），可以理解为 $A$ 在其特征向量上投影的叠加。

### Bonus: 主成分分析

## 二次型（quadratic form）
&emsp;&emsp;二次型是一种特殊的线性变换，（修改一下）可定义在 $\mathbb{R}^n$ 上的一类函数，表达式为 $Q(\pmb{x})=\pmb{x}^TA\pmb{x}$，其中 $A$ 是对称矩阵，比如：

$$Q(\pmb{x})=\begin{bmatrix}
   x_1 & x_2 \end{bmatrix}
\begin{bmatrix}
   1 & -4 \\
   -4 & -5
\end{bmatrix}
\begin{bmatrix}
   x_1 \\ x_2
\end{bmatrix}=x_1^2-8x_1x_2-5x_2^2
$$

&emsp;&emsp;若 $A=I$ 就可得到最简单的二次型 $Q(\pmb{x})=\pmb{x}^TI\pmb{x}=\parallel\pmb{x}\parallel$，显然，没有交叉项的二次型（$A$ 为对角阵）具备更加简洁的性质。幸运的是，我们可以通过变量替换的方式消去一般二次型中的交叉项。

&emsp;&emsp;考虑变量替换 $\pmb{x}=P\pmb{y}$，$Q(\pmb{x})=\pmb{x}^TA\pmb{x}=\pmb{y}^TP^TAP\pmb{y}$，问题转换为使 $P^TAP$ 对角化，这对于对称矩阵 $A$ 来说是显然的，其可以通过正交对角化为 $PDP^T$ 求出我们需要的 $P$。

$$\det(A-\lambda I)=0\Rightarrow\begin{cases}
   \lambda_1=3 \\ \lambda_2=-7
\end{cases}\Rightarrow P=\begin{bmatrix}
   2/\sqrt{5} & 1/\sqrt{5} \\
   -1/\sqrt{5} & 2/\sqrt{5}
\end{bmatrix}
$$

![quadratic.png](https://i.loli.net/2020/11/20/R9bsEqW4nBO5djr.png)

&emsp;&emsp;当对称阵 $A_{n\times n}$ 满足 $\forall\pmb{x}\in\mathbb{R}^n$，$\pmb{x}^TA\pmb{x}>0$ 时，$A$ 又被称作正定矩阵，它有以下性质：
* 所有的特征值 $\lambda$ 大于零
* 所有的子行列式大于零
* 所有主元大于零

&emsp;&emsp;在微积分中黑塞矩阵（Hessian matrix）的正定型与函数极值的存在性有关。

### 主轴定理（principal axis theorem）
关于积分等

### 条件优化（带限二次型最值问题）
&emsp;&emsp;工程中遇到的所有关于二次型最值优化问题，均可归结为单位向量在二次型中的最值问题，即：

$$\begin{aligned}
   &\max\quad Q(\pmb{x}) \\
   &s.t.\quad
   \pmb{x}^T\pmb{x}=1
\end{aligned}$$

&emsp;&emsp;对于无交叉项的二次型我们很容易就能求得它们的最值，且与其对应的主轴长相关。考虑一般类二次型 $Q(\pmb{x})=\pmb{x}^TA\pmb{x}=\pmb{y}^TD\pmb{y}$，注意到 $\parallel\pmb{x}\parallel=\parallel P\pmb{y}\parallel=\parallel\pmb{y}\parallel$（粗略地说，单位正交基仅起到旋转作用），因此求一般二次型的最值等价于求变量替换后得到的无交叉项二次型的最值。记 $\pmb{x}=P\pmb{y}$，$P=[\pmb{u}_1,\pmb{u}_2,\pmb{u}_3]$，若 $D$ 按特征值降序排列组成，则对于无交叉项二次型来说，取得最值的向量为 $\pmb{y}=[1,0,0]^T=\pmb{e}_1$，最值为 $\lambda_{\text{max}}=\lambda_1$。

$$\pmb{e}_1^TD\pmb{e}_1=\pmb{u}_1A\pmb{u}_1=\lambda_1$$

&emsp;&emsp;容易证明，二次型的取值范围与其对应主轴有关：

$$\begin{cases}
   \pmb{x}^TA\pmb{x} \\
   \pmb{x}^T\pmb{x}=1
\end{cases}\Rightarrow\lambda_{\text{min}}\leq Q(\pmb{x})\leq\lambda_{\text{max}}   
$$

&emsp;&emsp;增加更多关于。。。限制？咋写比较好



$$\begin{aligned}
   &\max\quad Q(\pmb{x}) \\
   &s.t.\quad\begin{cases}
      \pmb{x}^T\pmb{x}=1 \\
      \cdots\cdots \\
      \pmb{x}T\pmb{u}_{k-1}=0
   \end{cases}
\end{aligned}$$

&emsp;&emsp;由于 $\pmb{x}\in\text{span}(\pmb{u}_k,\dots,\pmb{u}_n)$，不难证明，此时 $\max{(\pmb{x}^TA\pmb{x})}=\lambda_k$。

## 奇异值分解（singular value decomposition）
&emsp;&emsp;我们已经知道，

### Bonus: 伪逆（pseudo-inverse）
$AA^+=U_rU_r^T$ 是 $C(A)$ 的投影阵

$A^+A=V_rV_r^T$ 是 $C(A^T)$ 的投影阵

若 $A\pmb{x}=\pmb{b}$ 相容，则 $\pmb{x}^+=A^+\pmb{b}$ 是最小长度解；不相容则是最小长度最小二乘解
