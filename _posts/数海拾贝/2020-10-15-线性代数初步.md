---
title: 线性代数初步（持续更新中）
date: 2020-10-15 17:27:05
categories: 
- 数学
tags:
- 数学
- 线性代数
---

![Matrix.jpg](https://i.loli.net/2020/10/19/IfjB8cOVxsu91ng.jpg)

# 矩阵（matrix）
## 向量、矩阵与线性变换
&emsp;&emsp;向量（vector）是标量的有序组合，在数理学科中常作为某个基本量的显式表达。如位置向量 $\pmb{x}=[2,3,5]^T$，由三维坐标显式指代；再如 IRIS 数据集内，我们用花萼长度，花萼宽度，花瓣长度，花瓣宽度四个分量组成的向量 $\pmb{v}=[L_1,W_1,L_2,W_2]^T$ 显式指代某朵具体的鸢尾花。向量与欧氏空间内的点（point）一一对应，其丰富的几何性质赋予了向量简洁的运算规则，这就是引入向量的根本原因。

&emsp;&emsp;在欧式空间的视角下，向量可以看作多个基底（basis）的加权组合，我们可以用矩阵乘法来表述这种关系：

$$
\left[\begin{matrix}
   3 \\ 2
\end{matrix}\right]
=3
\left[\begin{matrix}
   1 \\ 0
\end{matrix}\right]
+2
\left[\begin{matrix}
   0 \\ 1
\end{matrix}\right]
=
\left[\begin{matrix}
   1 & 0 \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   3 \\ 2
\end{matrix}\right]
$$

&emsp;&emsp;考虑向量 $\pmb{b_1}=[1,0]^T$，$\pmb{b_2}=[1,1]^T$ ，以 $B=[\pmb{b_1}, \pmb{b_2}]$ 为新基底，很容易得出原始坐标 $(3,2)$ 在新基构成的坐标系下的新坐标为 $(1,2)$，同理这种对应关系可以扩展至整个 $\mathbb{R}^2$ 平面。

![坐标变换.png](https://i.loli.net/2020/10/15/ZIy82uP5ifLe3Dj.png)

$$
\left[\begin{matrix}
   1 & 0 \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   3 \\ 2
\end{matrix}\right]
=
\left[\begin{matrix}
   1 & 1 \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   1 \\ 2
\end{matrix}\right]
\Rightarrow
\left[\begin{matrix}
   1 & 0 \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   x+y \\ y
\end{matrix}\right]
=
\left[\begin{matrix}
   1 & 1 \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   x \\ y
\end{matrix}\right]
$$

&emsp;&emsp;可以看出，欧式空间下的同一点，在不同基的表示下会得出不同的坐标，据此我们可以得出欧氏空间内的坐标变换公式： $\pmb{x}=P_B\pmb{x_B}$。同时我们也揭示了左乘矩阵的本质，即**换基，显式的建立欧氏空间内不同基表示的点的坐标间的映射关系**。

&emsp;&emsp;在计算机图形学领域内，我们能更加直观的理解矩阵变换所表示的这种映射关系，如剪切变换、旋转变换等。

![旋转和剪切变换.png](https://i.loli.net/2020/10/15/wWO6HBvLYcstr9V.png)

&emsp;&emsp;在 SLAM 中一个最基本的应用便是如何实现坐标平移。由于单个矩阵严格意义上只能实现拉伸或旋转，无法通过复合（composition）的方式实现整个平面的平移（原点位置无法改变），因此 SLAM 中引入了齐次坐标的概念，通过扩维实现某种意义上的“坐标平移”。（事实上剪切变换就实现了二维中单个坐标的平移）

$$
\left[\begin{matrix}
   R & T \\
   0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   x \\ y \\ 1
\end{matrix}\right]
=
\left[\begin{matrix}
   1 & 0 & h \\
   0 & 1 & k \\
   0 & 0 & 1
\end{matrix}\right]
\left[\begin{matrix}
   x \\ y \\ 1
\end{matrix}\right]
=
\left[\begin{matrix}
   x+h \\ y+k \\ 1
\end{matrix}\right]
$$

&emsp;&emsp;因此矩阵，**其实质就是针对向量进行的一种特殊的线性变换**。考虑变换 $T(\pmb{x})=A_{m\times n}\pmb{x}$，该变换以 $A$ 的列向量为基底建立了 $\mathbb{R}^n\rightarrow\mathbb{R}^m$ 的映射关系。这种变换与函数类似，均具有线性（$f+g$）和可复合性（$f\circ g$），这也是矩阵加法和乘法的意义所在。

## 矩阵方程与线性方程组
&emsp;&emsp;本科阶段的学习大多从线性方程组的解来引出矩阵乘法的含义，从解析式上看线性方程组的解与矩阵方程的解无疑是等价的。

$$
\begin{cases}
   x-2z=1 \\ 
   x+2y=5 \\
   y+z=2
\end{cases}
\Rightarrow
\left[\begin{matrix}
   1 & 0 & -2 \\
   1 & 2 & 5 \\
   0 & 1 & 1
\end{matrix}\right]
\left[\begin{matrix}
   x \\ y \\ z
\end{matrix}\right]
=
\left[\begin{matrix}
   1 \\ 5 \\ 2
\end{matrix}\right]
$$

&emsp;&emsp;考虑右侧的矩阵方程 $A\pmb{x}=\pmb{b}$，它表示以 $A$ 的列向量为基底对 $\pmb{x}$ 进行的线性变换。记 $A=[\pmb{c_1},\pmb{c_2}]$，为方便表述我们称由矩阵列向量张成的子空间为列空间（column space），即满足 $\pmb{y}=A\pmb{x}$ 的 $\pmb{y}$ 的集合。方程可解即可表示为 $b\in\text{span}\{\pmb{c_1},\pmb{c_2}\}$，即 $\pmb{b}\in C(A)$。通过高斯消元法对增广矩阵（augmented matrix）进行初等行变换即可得到方程的解。

$$
\left[\begin{array}{ccc|c}
   1 & 0 & -2 & 1\\
   1 & 2 & 5 & 5\\
   0 & 1 & 1 & 2\\
\end{array}\right]\rightarrow
\left[\begin{array}{ccc|c}
   1 & 0 & -2 & 1\\
   0 & 1 & 1 & 2\\
   0 & 0 & 0 & 0\\
\end{array}\right]\qquad
$$

$$
\pmb{x}=
\left[\begin{matrix}
   1 \\ 2 \\ 0
\end{matrix}\right]+
k\left[\begin{matrix}
   2 \\ -1 \\ 0
\end{matrix}\right],k\in\mathbb{R}
$$

&emsp;&emsp;再考虑齐次方程 $A\pmb{x}=\pmb{0}$，显然行变换不会影响等式右侧的零，最终得到的解为 $\pmb{x}=[2k,-k,0]^T$，与 $A\pmb{x}=\pmb{b}$ 的解集相比仅差了一个常向量（constant vector）。我们称所有满足 $A\pmb{x}=\pmb{0}$ 的 $\pmb{x}$ 的集合为 $A$ 的零空间（null space），$A\pmb{x}=\pmb{b}$ 的解空间可看作零空间的整体平移，即两个空间存在一个恒等的偏差（bias）。

&emsp;&emsp;对于 $A$ 变换后的上三角矩阵 $U_{m\times n}$，称对角线上的非零元为主元（pivot），称零元为自由变量（free variables），我们很容易就能找到一个关于秩和零空间的著名定理：秩-零度定理（rank-nullity theorem）

$$\text{rank}A+N(A)=\#\text{pivot}+\#\text{free variables}=m$$

&emsp;&emsp;以上，我们通过线性方程组引入了矩阵的两个重要性质：列空间与零空间。将 $A$ 改写为 $[r_1,r_2,r_3]^T$，对转置后的矩阵引入行空间（row space）与左零空间（column space of A transpose）的概念，敏锐的读者很快就能发现，矩阵的行空间与零空间事实上是正交的（orthogonal）。

$$
A\pmb{x}=\pmb{0}\rightarrow
\left[\begin{matrix}
   \pmb{r_1} \\ \pmb{r_2} \\ \pmb{r_3}
\end{matrix}\right]\pmb{x}=
\left[\begin{matrix}
   \pmb{r_1\cdot x} \\ \pmb{r_2\cdot x} \\ \pmb{r_3\cdot x}
\end{matrix}\right]=
\left[\begin{matrix}
   0 \\ 0 \\ 0
\end{matrix}\right]
$$

&emsp;&emsp;由于行空间与零空间的正交互补性，结合秩-零度定理，我们可以得到关于秩的一个经典结论：**行秩等于列秩**。

![四个基本子空间.jpg](https://i.loli.net/2020/10/19/tGgfES3FpweQbD4.png)

|基本子空间|相关性质
|:-:|:-:
|列空间 $C(A)$|$\dim C(A)=r=\#\text{pivot}$
|零空间 $N(A)$|$\dim N(A)=m-r=\text{\#free variables}$
|行空间 $C(A^T)$|$\dim C(A^T)=r$
|左零空间 $N(A^T)$|$\dim{N(A^T)}=n-r$

&emsp;&emsp;一言蔽之，矩阵 $A_{m\times n}$ 描述了线性变换 $T:\mathbb{R}^m\rightarrow\mathbb{R}^n$ ，其蕴含的四个基本子空间囊括了矩阵变换的本质特征。

## 线性变换?
&emsp;&emsp;考虑空间 $V:\mathbb{R}^n$ 与 $W:\mathbb{R}^m$，选取 $V$ 上的一组基 $B={\pmb{b}_1,\pmb{b}_2,\dots,\pmb{b}_n}$，对 $V$ 内任意向量 $\pmb{x}$，我们有 $\pmb{x}=r_1\pmb{b}_1+\dots+r_n\pmb{b}_n$。记 $\pmb{x}_B=[r_1,\dots,r_n]^T$，即 $\pmb{x}$ 在基 $B$ 下的坐标。我们知道，矩阵 $A_{m\times n}$ 描述了线性变换 $T:\mathbb{R}^n\rightarrow\mathbb{R}^m$，则矩阵对向量的作用可表示为：

$$T(\pmb{x})=r_1T(\pmb{b}_1)+\dots+r_nT(\pmb{b}_n)=M\pmb{x}_B$$

&emsp;&emsp;其中 $M=[T(\pmb{b}_1),\dots,T(\pmb{b}_n)]$。可以见得，空间到空间的线性变换，可以借由空间内的一组基来显式表示，这一点在欧式空间内的坐标变换公式 $\pmb{x}=P_B\pmb{x}_B$ 上体现的尤为明显。对于变换后的 $T(\pmb{x})$，可以取 $W$ 内的一组基 $C$ 来表示，即 $T(\pmb{x})_C=M\pmb{x}_B$。

![线性变换.jpg](https://i.loli.net/2020/10/27/JdzE5mbXwtxeVGs.jpg)

&emsp;&emsp;我们记 $\text{Im}T$ 为经过变换 $T$ 后的像（image），$\ker T$ 是像为零元的子空间（kernel，即零空间），如此即可以从线性变换的视角来理解秩-零度定理。

![Rank-nullity.png](https://i.loli.net/2020/10/20/eDFSA6MU1JOY7mZ.png)

&emsp;&emsp;再例如多项式上的微分算子事实上也是一种线性变换。取多项式空间上的一组基 $B={1,x,x^2,\dots,x^n}$，则微分算子可以表示为： 

$$D=[\frac{\partial1}{\partial x}_B,\dots,\frac{\partial x^n}{\partial x}_B]=
\left[\begin{matrix}
   0 & 1 & 0 &  \cdots & 0\\
   0 & 0 & 2  & \cdots & 0 \\
   0 & 0 & 0  & \ddots & \vdots \\
   \vdots & \vdots & \vdots & \ddots & n-1 \\
   0 & 0 & 0 & \cdots & 0
\end{matrix}\right]
$$

&emsp;&emsp;综上所述，同一个线性变换，可以用不同的基来显式表示，事实上这些基之间的关系互为同构（isomorphism）。

## 特征向量
&emsp;&emsp;在欧氏空间的视角下，矩阵变换可视作对空间内所有的点进行拉伸和旋转，而对于复合后的矩阵（如矩阵的幂），我们很难借助图像来表征这样的复合变换。

&emsp;&emsp;但假设存在这样的向量，矩阵对其只进行拉伸变换，我们就可以用拉伸系数的幂来表示矩阵的复合变换。我们称满足 $A\pmb{x}=\lambda\pmb{x}$ 的非零向量 $\pmb{x}$ 为特征向量（eigen vector），拉伸系数 $\lambda$ 为特征值（eigen value）；关于如何求解特征向量及特征值，在此不作赘述。将特征向量写成如下格式，我们很快就能得到关于特征向量的一个重要结论：**对角化定理**。

$$
\begin{cases}
   A\pmb{x}_1=\lambda_1\pmb{x}_1 \\ 
   A\pmb{x}_2=\lambda_2\pmb{x}_2 \\
   A\pmb{x}_3=\lambda_3\pmb{x}_3
\end{cases}\Rightarrow
A\left[\begin{matrix}
   \pmb{x}_1 & \pmb{x}_2 & \pmb{x}_3
\end{matrix}\right]=
\left[\begin{matrix}
   \pmb{x}_1 & \pmb{x}_2 & \pmb{x}_3
\end{matrix}\right]
\left[\begin{matrix}
   \lambda_1 & 0 &  \cdots & 0\\
   0 & \lambda_2  & \cdots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & 0 & \lambda_n
\end{matrix}\right]
$$

&emsp;&emsp;容易证明，不同特征值对应的特征向量间线性无关（一个方向只有一个拉伸系数），因此由特征向量组成的矩阵 $P$ 一定是可逆的，因此有 $A=PDP^{-1}$，其中 $D=\text{diag}\{\lambda_1,\dots,\lambda_n\}$，这就是矩阵的**特征值分解**。

&emsp;&emsp;我们已经知道，矩阵对应的线性变换可以借助不同的基来显式表示（通常由列向量组成的基表示），若选取特征向量组成的基 $B=\{\pmb{x}_1,\dots,\pmb{x}_n\}$，结合坐标变换 $\pmb{x}=P\pmb{x}_B$，我们有：

$$
\begin{aligned}
T_B &= [T(\pmb{x}_1)_B,T(\pmb{x}_2)_B,\dots,T(\pmb{x}_n)_B] \\
  &= [(\lambda_1\pmb{x}_1)_B,(\lambda_2\pmb{x}_2)_B,\dots,(\lambda_n\pmb{x}_n)_B] \\
  &= [P^{-1}A\pmb{x}_1,P^{-1}A\pmb{x}_2,\dots,P^{-1}A\pmb{x}_n] \\
  &= P^{-1}AP=D
\end{aligned}
$$

&emsp;&emsp;如上，我们知晓 $\pmb{x}\mapsto A\pmb{x}$ 和 $\pmb{u}\mapsto D\pmb{u}$ 事实上是等价的，仅仅是同一个线性变换的不同基的表示形式。我们称满足 $A=PBP^{-1}$ 的矩阵 $A,B$ 互为相似矩阵（similar matrix）。同时我们也会发现，使用特征向量作为基来描述矩阵表示的线性变换，在计算上会带来极大的便利。

&emsp;&emsp;遗憾的是，并不是所有的矩阵都有完备的特征向量组，对于这些矩阵我们另有新的方式进行求解（奇异值分解），这里暂且不表。

## 动力系统
&emsp;&emsp;考虑如下应用场景：若每年要统计一个城市及其郊区的人口分布问题，如 $x_0=[0.60,0.40]^T$ 表示初始有 $60\%$ 的人口住在城市，$40\%$ 的人口住在郊区。假设每年有 $5\%$ 的城市人口流动到郊区，有 $3\%$ 的人口流动到城市，我们可以用矩阵方程来表征上述一阶差分动力系统：

$$\pmb{x}_{k+1}=M\pmb{x}_k=
\left[\begin{matrix}
   0.95 & 0.03 \\
   0.05 & 0.97
\end{matrix}\right]^{k+1}
\left[\begin{matrix}
   0.60 \\
   0.40
\end{matrix}\right]
,\quad k=0,1,2\dots$$

&emsp;&emsp;令 $\det(A-\lambda I)=0$，可以求出 $M$ 的特征值 $\lambda_1=1$，$\lambda_2=0.92$，回代即可求得其对应的特征向量 $\pmb{v}_1=[3,5]^T$，$\pmb{v}_2=[1,-1]^T$。以特征向量为基底表示初始向量，我们就可以很快求出矩阵的幂对原始向量的影响：

$$\pmb{x}_1=A\pmb{x}_0=c_1\lambda_1\pmb{v}_1+c_2\lambda_2\pmb{v}_2=0.125\pmb{v}_1+0.225\cdot(0.92)^k\pmb{v}_2$$

$$\lim_{k\rightarrow\infty}\pmb{x}_k=0.125\pmb{v}_1=\left[\begin{matrix}
   0.375 \\
   0.625
\end{matrix}\right]$$

&emsp;&emsp;如上，利用矩阵的特征值分解我们可以快速求出矩阵的幂对向量的影响。事实上这是一个由一阶差分方程刻画的经典的马尔可夫链（Markov chain），该链经长期行为后最终收敛至稳态。

&emsp;&emsp;对于类似的一阶差分方程，我们亦可以借助点的轨迹来模拟动力系统的变化情况。下图模拟了三类经典的动力系统所代表的点的轨迹。

$$
A=\left[\begin{matrix}
   0.8 & 0 \\
   0 & 0.64
\end{matrix}\right]\quad
\pmb{x}_0=
\left[\begin{matrix}
   \pm0.2 \\
   \pm0.3
\end{matrix}\right]\quad
\text{吸引子（Attractor）}
$$

![track1.png](https://i.loli.net/2020/10/28/xe8wcEBKOdnstji.png)

$$
A=\left[\begin{matrix}
   1.44 & 0 \\
   0 & 1.2
\end{matrix}\right]\quad
\pmb{x}_0=
\left[\begin{matrix}
   \pm0.002 \\
   \pm0.0005
\end{matrix}\right]\quad
\text{排斥子（Repellent）}
$$

![track2.png](https://i.loli.net/2020/10/28/EUuS4GkwKtMZo6I.png)

$$
A=\left[\begin{matrix}
   2 & 0 \\
   0 & 0.5
\end{matrix}\right]\quad
\pmb{x}_0=
\left[\begin{matrix}
   \pm0.01 \\
   \pm1
\end{matrix}\right]\quad
\text{鞍点（Saddle point）}
$$

![track3.png](https://i.loli.net/2020/10/28/wupzdIJBxorqcR1.png)

## 
