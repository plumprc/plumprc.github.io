---
title: 概率论和随机过程（更新中）
date: 2021-01-23 23:35:21
categories: 
- 数学
tags:
- 数学
- 概率论
---

# 概率（probability）
## 样本空间（sample space）
&emsp;&emsp;概率描述的是相对频率，用以研究相继发生或同时发生的大量事件的平均特性。[贝特朗投针实验](https://baike.baidu.com/item/%E8%B4%9D%E7%89%B9%E6%9C%97%E6%82%96%E8%AE%BA/9241081)告诉我们，探讨某一事件概率的前提是确定**样本空间**，不同的样本空间所对应的事件概率可能是不同的。一般来说，随机试验的所有样本点组成的集合 $S$ 称为样本空间，样本空间的任意子集称为随机事件。

&emsp;&emsp;**例 1**：工人生产三个零件，以事件 $A_i(i=1,2,3)$ 表示他生产的第 $i$ 个零件是合格品，那么

|事件|结果表示
|:-:|:-:
|至少有一个合格品|$A_1\cup A_2\cup A_3$
|最多只有两个合格品|$\overline{A_1A_2A_3}$

&emsp;&emsp;现代概率论的基础是苏联数学家柯尔莫格罗夫提出的公理化定义（axiom），结合集合论我们很容易就能得出诸如差事件或容斥定理那样的概率论公式。
* 任意事件 $A$ 满足 $P(A)\geq0$
* 必然事件的概率为一，即 $P(S)=1$
* 互斥事件 $A,B$ 满足 $P(A\cup B)=P(A)+P(B)$

&emsp;&emsp;在假定事件 $B$ 发生的情况下，事件 $A$ 发生的概率称为条件概率（conditional probability），记作 $P(A\vert B)$。条件概率相当于对原始样本空间进行了缩减，将 $S$ 简化为由 $B$ 事件组成的子集，因此适用于普通概率的公式同样也适用于条件概率。

$$P(A\vert B)=\frac{P(AB)}{P(B)},\quad P(B)\ne0$$

&emsp;&emsp;进一步，若 $\{A_1,A_2,\dots,A_n\}$ 是 $S$ 的分割（division），我们可以写出任意事件 $B$ 基于该分割的展开式，该式是全概率定理（total probability theorem）的表现形式。

$$P(B)=\sum_{i=1}^n{P(A_i)P(B\vert A_i)}$$

&emsp;&emsp;结合条件概率和全概率公式我们可以推导出著名的贝叶斯定理（Bayes's theorem），它的一般形式如下，其中 $H$ 表假设（hypothesis），$E$ 表事实（evidence）。 

$$P(H\vert E)=\frac{P(H)P(E\vert H)}{P(E)}=\frac{P(H)P(E\vert H)}{P(H)P(E\vert H)+P(\bar{H})P(E\vert\bar{H})}$$

> “New evidence does not completely determine your beliefs in a vaccum, it should update prior beliefs.”

&emsp;&emsp;贝叶斯定理的内核如上述所言，我们对样本空间 $S$ 提出了假设 $H$，根据新的事实 $E$ 对假设进行更新（update），这里 $E$ 同样起到了缩减样本空间的作用。$P(H)$ 称为先验概率（prior），$P(E\vert H)$ 称为似然概率（likelihood），$P(H\vert E)$ 称为后验概率（posterior）。

&emsp;&emsp;**例 2**：有 $n$ 枚硬币，$n-1$ 枚是均匀的，一枚两面都是正面，随机选一枚掷 $m$ 次都是正面，求选择的这枚硬币是均匀的概率。

$$P(H)=\frac{n-1}{n}\quad P(E\vert H)=\frac{1}{2^m}$$

$$P(E)=\frac{n-1}{n}\times\frac{1}{2^m}+\frac{1}{n}\times1=\frac{2^m+n-1}{n\cdot2^m}$$

$$P(H\vert E)=\frac{P(H)P(E\vert H)}{P(E)}=\frac{n-1}{2^m+n-1}$$

&emsp;&emsp;**例 3**：医院检测一项疾病，当地经排查得知该病的患病率为 $2\%$。已知这个检查诊断正确率为 $99\%$（即得病的人中有 $9\%$ 概率漏诊，没病的人有 $9\%$ 误诊），此时我们发现了一个人检测为阳性，问此人患病的概率。

$$P(H)=0.02\quad P(E\vert H)=0.95$$

$$P(E)=0.02\times0.95+0.98\times0.05=0.068$$

$$P(H\vert E)=\frac{P(H)P(E\vert H)}{P(E)}=0.279$$

&emsp;&emsp;试剂的准确率主要源自患病人群的筛查，因此 $E$ 的引入修正了我们对样本空间的错误认知。针对更广泛的群体，试剂的高灵敏度一定伴随着部分假阳性问题，所以对于传染性疾病复检是很有必要的。

&emsp;&emsp;有时候我们会关心多个事件同时或连续进行（取交集）的概率，因此引入事件独立性的概念是很有必要的。独立事件 $A,B$ 满足 $P(AB)=P(A)P(B)$，意及两事件间互不影响，注意独立与互斥没有必然联系。

&emsp;&emsp;**例 4**：掷一枚不均匀硬币 $n$ 次，求出现 $k$ 次正面的概率（单次正面概率为 $p$）。

$$P=\binom{n}{k}p^k(1-p)^{n-k}$$

&emsp;&emsp;这是一个经典的 $n$ 重伯努利试验（Bernoulli experiment），它描述了 $n$ 次独立实验中事件 $A$ 成功或失败次数的分布情况。

## 随机变量（random variable）
### 一维随机变量及其分布
&emsp;&emsp;随机变量是样本空间 $S$ 上的函数 $f:S\rightarrow\mathbb{R}$ ，记为 $X(s)$，$s\in S$。随机变量的分布函数（probability distribution function）定义为 $F(x)=P\{X\leq x\}$，描述了随机变量落入某特定区间的概率，它满足以下性质：
* 分布函数非减，且处处右连续
* $F(-\infty)=0$，$F(+\infty)=1$
* $P\{a<X\leq b\}=F(b)-F(a)$，$P\{X=x_0\}=F(x_0)-F(x_0-0)$

&emsp;&emsp;分布函数描述了随机变量 $X$ 的统计性质，分布函数的导数被称为概率密度函数（probability density function），记作 $f_X(x)$。

$$f_X(x)=\frac{\mathrm{d}F(x)}{\mathrm{d}x}\qquad F(x)=\displaystyle\int_{-\infty}^xf_X(t)\mathrm{d}t$$

&emsp;&emsp;显然连续型随机变量的分布函数必为连续函数，而离散型随机变量的分布函数呈阶跃函数，因此其密度函数可用冲激函数表示，这就是随机信号分析中的概率质量函数（probability mass function）。

$$f_X(x)=\sum_ip_i\delta(x-x_i)$$

&emsp;&emsp;随机变量刻画了样本空间内所有事件取值的可能性，因此事件的统计特征（如均值）即可用随机变量的统计特征来描述。随机变量 $X$ 的期望（expectation）定义如下：

$$E(X)=\int_{-\infty}^\infty xf_(x)\mathrm{d}x=\lim_{\Delta\rightarrow0}\sum_{k=-\infty}^\infty x_kf(x_k)\Delta x=\int_SX\mathrm{d}P$$

&emsp;&emsp;期望描述了大量事件发生时的平均特性，黎曼和（Riemann sum）告诉我们，当实验次数 $n\rightarrow\infty$ 时，随机变量观测值的算数平均值趋向于期望。事实上，期望可看做样本空间内所有子事件与之概率（测度）之积的和，即 $X$ 的勒贝格积分（Lebesgue integral）。容易证明，期望的计算是满足线性性质的。

$$E(aX+bY)=aE(X)+bE(Y)$$

&emsp;&emsp;方差（variance/deviation）描述了随机变量与其期望的偏离/集中程度，方差越大偏离程度越大。（无特殊说明，我们习惯用 $\mu$ 表示均值）

$$D(X)=E((X-\mu)^2)=\int_{-\infty}^\infty (x-\mu)^2f_(x)\mathrm{d}x$$

&emsp;&emsp;如果把 $f(x)$ 解释为概率在 $x$ 轴上的质量分布函密度，那么 $E(X)$ 就是它的重心，$E(X^2)$ 是它对原点的惯性矩，$D(X)$ 则是其对重心的转动惯性矩，标准差 $\sigma$ 表示回转半径。期望和方差仅仅给出了 $f(x)$ 的一些特性，其他性质则隐藏在其他的矩（moment）信息里。以下给出了常用的 $n$ 阶矩和 $n$ 阶中心距（central momen）的公式，注意期望实质就是 $f(x)$ 的一阶矩，而方差是其的二阶中心距。

$$m_n=E(X^n)=\int_{-\infty}^\infty x^nf(x)\mathrm{d}x$$

$$\mu_n=E((X-E(X))^n)=\int_{-\infty}^\infty(x-E(X))^nf(x)\mathrm{d}x$$

&emsp;&emsp;对概率密度函数做共轭傅里叶变换就可以用密度函数的 $n$ 阶矩特征来表示其性质，注意 $\Phi(\omega)$ 在原点的导数等于 $X$ 的各阶矩，因此 $\Phi(\omega)$ 又被称为矩函数。

$$\Phi_X(\omega)=\int_{-\infty}^\infty f(x)e^{j\omega x}\mathrm{d}x=E(e^{j\omega x})$$

$$E(e^{j\omega x})=1+\frac{j\omega E(X)}{1!}+\frac{(j\omega)^2E(X^2)}{2!}+\dots+\frac{(j\omega)^nE(X^n)}{n!}$$

$$f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty\Phi_X(\omega)e^{-j\omega x}\mathrm{d}\omega\qquad\Phi^{(n)}(0)=m_n$$

&emsp;&emsp;**例 5**（卷积定理）：已知 $X,Y$ 是独立的随机变量，且 $Z=X+Y$，求 $Z$ 的概率密度函数。

$$\Phi_Z(\omega)=\Phi_X(\omega)\cdot\Phi_Y(\omega)$$

$$f_Z(z)=f_X(x)*f_Y(y)$$

&emsp;&emsp;可以见得，特征函数的引入大大简化了复合型随机变量的运算，原本多个独立随机变量的和需要进行复杂的卷积运算，而转换为特征函数后只需进行简单的乘积然后进行反变换即可得出结果。由于傅里叶变换中变换前后的函数一一对应，因此对于 $Y=g(X)$ 类随机变量仅需通过配凑的方式即可得出密度函数。

&emsp;&emsp;**例 6**：已知 $X\sim N(0,1)$，$Y=X^2$，求 $f_Y(y)$。

$$\Phi_Y(\omega)=\int_{-\infty}^\infty f(x)e^{j\omega x^2}\mathrm{d}x=\frac{2}{\sqrt{2\pi}}\int_0^\infty e^{j\omega y}e^{-\frac{y}{2}}\frac{1}{2\sqrt{y}}\mathrm{d}y$$

$$f_Y(y)=\frac{1}{\sqrt{2\pi y}}e^{-\frac{y}{2}}U(y)$$

---

&emsp;&emsp;离散型随机变量中最简单的是伯努利随机变量，$X$ 仅取两个值 $0,1$。$n$ 重伯努利试验的结果就可以用二项分布（binomial distribution）来描述。

$$P\{X=k\}=\displaystyle\binom{n}{k}p^k(1-p)^{n-k}$$

&emsp;&emsp;有时我们会关注 $n$ 重伯努利试验至第一次成功所需的实验次数（例如掷骰子何时会出现 $6$），这可以用几何分布（geometric distribution）来描述。若假定前 $m$ 次没有成功，第一次成功出现在接下来的 $n$ 次试验的概率仅依赖 $n$，这说明几何分布具有无记忆性。

$$P\{X=k\}=(1-p)^{k-1}p\quad k=1,2,\dots$$

$$P\{X>m\}=\sum_{k=m+1}^\infty (1-p)^{k-1}p=(1-p)^m$$

$$P\{X>m+n\vert X>m\}=\frac{P\{X>m+n\}}{P\{X>m\}}=(1-p)^n$$

&emsp;&emsp;几何分布的一个显而易见的推广是考虑实现 $r$ 次成功所需的实验次数（例如掷两次 $6$ 所需的掷骰子次数），这可以用负二项分布（negative binomial distribution）来描述。

$$P\{X=k\}=\binom{k-1}{r-1}p^{r}(1-p)^{k-r}\quad k=r,r+1,\dots$$

&emsp;&emsp;倘若读者饶有兴致的画出随着 $n$ 增大时 $n$ 重伯努利试验的概率分布，你会惊奇的发现所画出的曲线会慢慢逼近于一个钟型分布曲线，这就是概率论历史上最为著名的正态分布（normal distribution）曲线。（推荐阅读：[正态分布的前世今生](https://cosx.org/2013/01/story-of-normal-distribution-1)）

$$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

&emsp;&emsp;如果说纷繁芜杂的概率支配了世间万物，那么正态分布将是一切随机现象的归途。

TODO: 中心极限定理

---

TODO: 泊松分布，大量实验中稀有事件发生的次数

$$P\{X=k\}=\displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$$

TODO: 多个独立同分布随机变量平均的极限服从正态分布

TODO: 正态近似、泊松近似

TODO: 指数分布和排队论、马尔科夫、无记忆性关系

$$f(x)=\lambda e^{-\lambda x}\quad x\geq0$$

---

### 大数定律和中心极限定理
在统计活动中，人们发现，在相同条件下大量重复进行一种随机试验时，事件发生的频率值会趋近于某一数值，这个就是最早的大数定律。一般大数定律讨论的是 n 个随机变量平均值的稳定性。

而中心极限定理则是证明了在很一般的条件下，n 个随机变量的和当 n 趋近于正无穷时的极限分布是正态分布

大数定律讲的是样本均值收敛到总体均值

而中心极限定理告诉我们，当样本足够大时，样本均值的分布会慢慢变成正态分布


我们引入指示函数（indicator function）$I_{(A)}$，示性函数只有在事件 A 成立时才返回 1，否则为 0。示性函数满足以下性质：

$$E(I_{(A)})=P(A)\qquad I_{(x\geq a)}\leq\frac{X}{a}$$

将自变量 $x$ 替换为随机变量 $X$，以上不等式也成立，对两边取均值即可得到 **马尔科夫不等式**。

$$P(X\geq a)\leq\frac{E(X)}{a}$$

对于**切比雪夫不等式**，也可以用类似的示性函数来几何直观证明。对于任意的 $x,a,b$，我们有

$$I_{(|x-a|\geq b)}\leq\frac{(x-a)^2}{b^2}$$

对不等式两边取均值，选取随机变量的均值作为 $a$ ，注意到 $D(X)=E((X-\mu)^2)$，我们有

$$P(|X-\mu|\geq b)\leq\frac{\sigma^2}{b^2}$$




### 二维随机变量及其分布

$$F(x,y)=P\{X\leq x,Y\leq y\}\qquad f(x,y)=\frac{\partial^2F(x,y)}{\partial x\partial y}$$

$$P\{(X,Y)\in D\}=\displaystyle\iint_Df(x,y)\mathrm{d}x\mathrm{d}y$$

&emsp;&emsp;二维随机变量的联合分布函数和密度函数与一维相仿，注意我们在关注多随机变量联合分布的同时，也会格外关注单个随机变量的统计特性。

$$f_X(x)=\int_{-\infty}^\infty f(x,y)\mathrm{d}y\qquad f_Y(y)=\int_{-\infty}^\infty f(x,y)\mathrm{d}x$$

&emsp;&emsp;二维随机变量的密度函数同样可看做 $\mathbb{R}^2$ 上的质量密度函数，期望同样表质心，相关矩的性质也与一维一致。

&emsp;&emsp;**例 7**：已知 $f(x,y)=1$，$0<y<x<1$，其他情况为 $0$，求 $E(X\vert Y)$ 和 $E(Y\vert X)$。

$$f_X(x)=\int_0^x1\mathrm{d}y=x\qquad f_Y(y)=\int_y^11\mathrm{d}x=1-y$$

$$f(x\vert y)=\frac{1}{1-y}\qquad f(y\vert x)=\frac{1}{x}$$

$$E(X\vert Y)=\int_{-\infty}^\infty xf(x\vert y)\mathrm{d}x=\int_y^1\frac{x}{1-y}\mathrm{d}x=\frac{1+y}{2}$$

$$E(Y\vert X)=\int_{-\infty}^\infty yf(y\vert x)\mathrm{d}y=\int_0^x\frac{y}{x}\mathrm{d}y=\frac{x}{2}$$

&emsp;&emsp;事实上 $E(Y\vert X)$ 描述了 $(x,x+\mathrm{d}x)$ 这个垂直带状质量密度函的质心轨迹，该轨迹又被称为回归线。

&emsp;&emsp;**例 8**：$Z=X/Y$，求 $F_Z(z)$。

$$P\{X/Y\leq z\}=P\{X\leq YZ,Y>0\}+P\{X\geq XY,Y<0\}$$

$$F_Z(z)=\int_0^\infty\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^0\int_{yz}^\infty f(x,y)\mathrm{d}x\mathrm{d}y$$

&emsp;&emsp;上例给出了形如 $Z=g(X,Y)$ 的复合型随机变量分布函数的求法，对于和式我们可以用卷积定理简单求解，对于更一般的情况我们采取分割概率空间的情况分别求积分。

&emsp;&emsp;**例 9**：$Z_1=\max(X,Y)$，$Z_2=\min(X,Y)$，求 $F_{Z_1}(z)$ 和 $F_{Z_2}(z)$。

$$F_{Z_1}(z)=P(X\leq z,Y\leq z)=F_X(z)\cdot F_Y(z)$$

$$F_{Z_2}(z)=1-P\{X>z,Y>z\}=F_X(z)+F_Y(z)-F_{XY}(z,z)$$

&emsp;&emsp;有时候我们会关注随机变量间的关系。独立性 $F(x,y)=F_X(x)F_Y(y)$ 说明了事件 $X,Y$ 互不影响，而相关性（correlation）则从空间上给出了事件间的位置关系。将随机变量抽象为向量，则 $E(XY)$ 可看做 $X,Y$ 的内积，柯西不等式在随机变量上的表现形式即：

$$E^2(XY)\leq E(X^2)E(Y^2)$$

&emsp;&emsp;定义协方差（covariance）$C(X,Y)$ 描述随机变量去均值后的位置关系，协方差为零则表示去均值后的随机变量相互正交（orthogonal），即不相关。相关系数 $\rho_{XY}$ 将协方差规范至 $[0,1]$ 上的实数。

$$C(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y)$$

$$\rho_{XY}=\frac{C(X,Y)}{D(X)D(Y)}$$

&emsp;&emsp;注意随机变量间独立则一定不相关，反之不成立。

# 随机过程（stochastic process）
